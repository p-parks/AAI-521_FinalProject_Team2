{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from IPython.display import display, Javascript, Image\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_dir = './Transfomer/model/'\n",
    "lstm_dir = './LSTM/model-top-15/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HAND_IDXS: 21, N_COLS: 66\n",
      "LIPS_START: 0, LEFT_HAND_START: 40, RIGHT_HAND_START: 61, POSE_START: 61\n"
     ]
    }
   ],
   "source": [
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# Dense layer units for landmarks\n",
    "LIPS_UNITS = 384\n",
    "HANDS_UNITS = 384\n",
    "POSE_UNITS = 384\n",
    "# final embedding and transformer embedding size\n",
    "UNITS = 512\n",
    "\n",
    "# Transformer\n",
    "NUM_BLOCKS = 2\n",
    "MLP_RATIO = 2\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.00\n",
    "MLP_DROPOUT_RATIO = 0.30\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
    "\n",
    "# Initiailizers\n",
    "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "GELU = tf.keras.activations.gelu\n",
    "\n",
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "IS_INTERACTIVE = True\n",
    "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
    "\n",
    "INPUT_SIZE = 64\n",
    "\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "\n",
    "USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
    "START_IDX = 468\n",
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
    "LANDMARK_IDXS_LEFT_DOMINANT0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, LEFT_POSE_IDXS0))\n",
    "LANDMARK_IDXS_RIGHT_DOMINANT0 = np.concatenate((LIPS_IDXS0, RIGHT_HAND_IDXS0, RIGHT_POSE_IDXS0))\n",
    "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
    "N_COLS = LANDMARK_IDXS_LEFT_DOMINANT0.size\n",
    "# Landmark indices in processed data\n",
    "LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LIPS_IDXS0)).squeeze()\n",
    "LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_HAND_IDXS0)).squeeze()\n",
    "RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, RIGHT_HAND_IDXS0)).squeeze()\n",
    "HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, HAND_IDXS0)).squeeze()\n",
    "POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_POSE_IDXS0)).squeeze()\n",
    "\n",
    "print(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')\n",
    "\n",
    "LIPS_START = 0\n",
    "LEFT_HAND_START = LIPS_IDXS.size\n",
    "RIGHT_HAND_START = LEFT_HAND_START + LEFT_HAND_IDXS.size\n",
    "POSE_START = RIGHT_HAND_START + RIGHT_HAND_IDXS.size\n",
    "\n",
    "print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')\n",
    "\n",
    "LIPS_MEAN = np.load(f'{transformer_dir}/LIPS_MEAN.npy')\n",
    "LIPS_STD = np.load(f'{transformer_dir}/LIPS_STD.npy')\n",
    "LEFT_HANDS_MEAN = np.load(f'{transformer_dir}/LEFT_HANDS_MEAN.npy')\n",
    "LEFT_HANDS_STD = np.load(f'{transformer_dir}/LEFT_HANDS_STD.npy')\n",
    "POSE_MEAN = np.load(f'{transformer_dir}/POSE_MEAN.npy')\n",
    "POSE_STD = np.load(f'{transformer_dir}/POSE_STD.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "    def get_diffs(self, l):\n",
    "        S = l.shape[2]\n",
    "        other = tf.expand_dims(l, 3)\n",
    "        other = tf.repeat(other, S, axis=3)\n",
    "        other = tf.transpose(other, [0,1,3,2])\n",
    "        diffs = tf.expand_dims(l, 3) - other\n",
    "        diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "        return diffs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Positional Embedding, initialized with zeros\n",
    "        self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "        # Embedding layer for Landmarks\n",
    "        self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
    "        self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
    "        self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
    "        # Landmark Weights\n",
    "        self.landmark_weights = tf.Variable(tf.zeros([3], dtype=tf.float32), name='landmark_weights')\n",
    "        # Fully Connected Layers for combined landmarks\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "            tf.keras.layers.Activation(GELU),\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name='fc')\n",
    "\n",
    "\n",
    "    def call(self, lips0, left_hand0, pose0, non_empty_frame_idxs, training=False):\n",
    "        # Lips\n",
    "        lips_embedding = self.lips_embedding(lips0)\n",
    "        # Left Hand\n",
    "        left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "        # Pose\n",
    "        pose_embedding = self.pose_embedding(pose0)\n",
    "        # Merge Embeddings of all landmarks with mean pooling\n",
    "        x = tf.stack((\n",
    "            lips_embedding, left_hand_embedding, pose_embedding,\n",
    "        ), axis=3)\n",
    "        x = x * tf.nn.softmax(self.landmark_weights)\n",
    "        x = tf.reduce_sum(x, axis=3)\n",
    "        # Fully Connected Layers\n",
    "        x = self.fc(x)\n",
    "        # Add Positional Embedding\n",
    "        max_frame_idxs = tf.clip_by_value(\n",
    "                tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True),\n",
    "                1,\n",
    "                np.PINF,\n",
    "            )\n",
    "        normalised_non_empty_frame_idxs = tf.where(\n",
    "            tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "            INPUT_SIZE,\n",
    "            tf.cast(\n",
    "                non_empty_frame_idxs / max_frame_idxs * INPUT_SIZE,\n",
    "                tf.int32,\n",
    "            ),\n",
    "        )\n",
    "        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LandmarkEmbedding(tf.keras.Model):\n",
    "    def __init__(self, units, name):\n",
    "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Embedding for missing landmark in frame, initizlied with zeros\n",
    "        self.empty_embedding = self.add_weight(\n",
    "            name=f'{self.name}_empty_embedding',\n",
    "            shape=[self.units],\n",
    "            initializer=INIT_ZEROS,\n",
    "        )\n",
    "        # Embedding\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "            tf.keras.layers.Activation(GELU),\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name=f'{self.name}_dense')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.where(\n",
    "                # Checks whether landmark is missing in frame\n",
    "                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                # If so, the empty embedding is used\n",
    "                self.empty_embedding,\n",
    "                # Otherwise the landmark data is embedded\n",
    "                self.dense(x),\n",
    "            )\n",
    "\n",
    "# Full Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(Transformer, self).__init__(name='transformer')\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(UNITS, 8))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(UNITS * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ]))\n",
    "\n",
    "    def call(self, x, attention_mask):\n",
    "        # Iterate input over transformer blocks\n",
    "        for mha, mlp in zip(self.mhas, self.mlps):\n",
    "            x = x + mha(x, attention_mask)\n",
    "            x = x + mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
    "# replaced softmax with softmax layer to support masked softmax\n",
    "def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "    #calculates Q . K(transpose)\n",
    "    qkt = tf.matmul(q,k,transpose_b=True)\n",
    "    #caculates scaling factor\n",
    "    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "    scaled_qkt = qkt/dk\n",
    "    softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "\n",
    "    z = tf.matmul(softmax,v)\n",
    "    #shape: (m,Tx,depth), same shape as q,k,v\n",
    "    return z\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_of_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.depth = d_model//num_of_heads\n",
    "        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self,x, attention_mask):\n",
    "\n",
    "        multi_attn = []\n",
    "        for i in range(self.num_of_heads):\n",
    "            Q = self.wq[i](x)\n",
    "            K = self.wk[i](x)\n",
    "            V = self.wv[i](x)\n",
    "            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "\n",
    "        multi_head = tf.concat(multi_attn,axis=-1)\n",
    "        multi_head_attention = self.wo(multi_head)\n",
    "        return multi_head_attention\n",
    "\n",
    "# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\n",
    "def scce_with_ls(y_true, y_pred):\n",
    "    # One Hot Encode Sparsely Encoded Target Sign\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_true = tf.one_hot(y_true, NUM_CLASSES, axis=1)\n",
    "    y_true = tf.squeeze(y_true, axis=2)\n",
    "    # Categorical Crossentropy with native label smoothing support\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25)\n",
    "\n",
    "def get_transformer_model():\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask0 = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask0 = tf.expand_dims(mask0, axis=2)\n",
    "    # Random Frame Masking\n",
    "    mask = tf.where(\n",
    "        (tf.random.uniform(tf.shape(mask0)) > 0.25) & tf.math.not_equal(mask0, 0.0),\n",
    "        1.0,\n",
    "        0.0,\n",
    "    )\n",
    "    # Correct Samples Which are all masked now...\n",
    "    mask = tf.where(\n",
    "        tf.math.equal(tf.reduce_sum(mask, axis=[1,2], keepdims=True), 0.0),\n",
    "        mask0,\n",
    "        mask,\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    # LIPS\n",
    "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 40, 2])\n",
    "    lips = tf.where(\n",
    "            tf.math.equal(lips, 0.0),\n",
    "            0.0,\n",
    "            (lips - LIPS_MEAN) / LIPS_STD,\n",
    "        )\n",
    "    # LEFT HAND\n",
    "    left_hand = tf.slice(x, [0,0,40,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    left_hand = tf.where(\n",
    "            tf.math.equal(left_hand, 0.0),\n",
    "            0.0,\n",
    "            (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "        )\n",
    "    # POSE\n",
    "    pose = tf.slice(x, [0,0,61,0], [-1,INPUT_SIZE, 5, 2])\n",
    "    pose = tf.where(\n",
    "            tf.math.equal(pose, 0.0),\n",
    "            0.0,\n",
    "            (pose - POSE_MEAN) / POSE_STD,\n",
    "        )\n",
    "\n",
    "    # Flatten\n",
    "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 40*2])\n",
    "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 5*2])\n",
    "\n",
    "    # Embedding\n",
    "    x = Embedding()(lips, left_hand, pose, non_empty_frame_idxs)\n",
    "\n",
    "    # Encoder Transformer Blocks\n",
    "    x = Transformer(NUM_BLOCKS)(x, mask)\n",
    "\n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classifier Dropout\n",
    "    x = tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "\n",
    "    # Sparse Categorical Cross Entropy With Label Smoothing\n",
    "    loss = scce_with_ls\n",
    "\n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_transformer = get_transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.load_weights(f'{transformer_dir}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "signmap_sub_dir = 'sign_to_prediction_index_map.json'\n",
    "signmap_full_file_path = os.path.join(transformer_dir, signmap_sub_dir)\n",
    "# Load the sign to index mapping\n",
    "with open(signmap_full_file_path, 'r') as file:\n",
    "    sign_to_index = json.load(file)\n",
    "\n",
    "inverted_mapping = {v: k for k, v in sign_to_index.items()}\n",
    "# Convert mapping to list\n",
    "class_names = [inverted_mapping[i] for i in sorted(inverted_mapping)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required files not found in the folder.\n"
     ]
    }
   ],
   "source": [
    "load_lstm = False\n",
    "h5_file = None\n",
    "npy_file = None\n",
    "\n",
    "for file in os.listdir(lstm_dir):\n",
    "    if file.endswith('.h5') and not h5_file:\n",
    "        h5_file = file\n",
    "    elif file.endswith('.npy') and not npy_file:\n",
    "        npy_file = file\n",
    "    if h5_file and npy_file:\n",
    "        break\n",
    "\n",
    "if h5_file and npy_file and load_lstm:\n",
    "    actions = np.load(os.path.join(lstm_dir, npy_file), allow_pickle=True)\n",
    "\n",
    "    # model = load_model(os.path.join(lstm_dir, h5_file))\n",
    "    num_classes = actions.shape[0]\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(10, 1662)))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(BatchNormalization())\n",
    "    model_lstm.add(LSTM(256, return_sequences=False, activation='relu'))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(128, activation='relu'))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_lstm.load_weights(os.path.join(lstm_dir, h5_file))\n",
    "    print(\"Model and actions loaded successfully.\")\n",
    "else:\n",
    "    print(\"Required files not found in the folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Transformer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = []\n",
    "TenDataFrame = []\n",
    "\n",
    "def transform(results, frame_number):\n",
    "  frame = []\n",
    "  type_ = []\n",
    "  index = []\n",
    "  x = []\n",
    "  y = []\n",
    "  z = []\n",
    "  #image.flags.writeable = False\n",
    "  #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  #results = holistic.process(image)\n",
    "  #face\n",
    "  if(results.face_landmarks is None):\n",
    "    for ind in range(468):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"face\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.face_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"face\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #left hand\n",
    "  if(results.left_hand_landmarks is None):\n",
    "    for ind in range(21):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"left_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.left_hand_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"left_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #pose\n",
    "  if(results.pose_landmarks is None):\n",
    "    for ind in range(33):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"pose\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.pose_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"pose\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #right hand\n",
    "  if(results.right_hand_landmarks is None):\n",
    "    for ind in range(21):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"right_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.right_hand_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"right_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "  #data = np.array([frame, type_, index, x, y, z])\n",
    "  return pd.DataFrame({\"frame\" : frame,\"type\"  : type_,\"landmark_index\" : index,\"x\" : x,\"y\" : y,\"z\" : z})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Tensorflow layer to process data in TFLite\n",
    "    Data needs to be processed in the model itself, so we can not use Python\n",
    "\"\"\"\n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PreprocessLayer, self).__init__()\n",
    "        normalisation_correction = tf.constant([\n",
    "                    # Add 0.50 to left hand (original right hand) and substract 0.50 of right hand (original left hand)\n",
    "                    [0] * len(LIPS_IDXS) + [0.50] * len(LEFT_HAND_IDXS) + [0.50] * len(POSE_IDXS),\n",
    "                    # Y coordinates stay intact\n",
    "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
    "                    # Z coordinates stay intact\n",
    "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "        self.normalisation_correction = tf.transpose(normalisation_correction, [1,0])\n",
    "\n",
    "    def pad_edge(self, t, repeats, side):\n",
    "        if side == 'LEFT':\n",
    "            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n",
    "        elif side == 'RIGHT':\n",
    "            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None,N_ROWS,N_DIMS], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, data0):\n",
    "        # Number of Frames in Video\n",
    "        N_FRAMES0 = tf.shape(data0)[0]\n",
    "\n",
    "        # Find dominant hand by comparing summed absolute coordinates\n",
    "        left_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1))\n",
    "        right_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1))\n",
    "        left_dominant = left_hand_sum >= right_hand_sum\n",
    "\n",
    "        # Count non NaN Hand values in each frame for the dominant hand\n",
    "        if left_dominant:\n",
    "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
    "                    tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1),\n",
    "                    axis=[1, 2],\n",
    "                )\n",
    "        else:\n",
    "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
    "                    tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1),\n",
    "                    axis=[1, 2],\n",
    "                )\n",
    "\n",
    "        # Find frames indices with coordinates of dominant hand\n",
    "        non_empty_frames_idxs = tf.where(frames_hands_non_nan_sum > 0)\n",
    "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
    "        # Filter frames\n",
    "        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n",
    "\n",
    "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
    "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32)\n",
    "        # Normalize to start with 0\n",
    "        non_empty_frames_idxs -= tf.reduce_min(non_empty_frames_idxs)\n",
    "\n",
    "        # Number of Frames in Filtered Video\n",
    "        N_FRAMES = tf.shape(data)[0]\n",
    "\n",
    "        # Gather Relevant Landmark Columns\n",
    "        if left_dominant:\n",
    "            data = tf.gather(data, LANDMARK_IDXS_LEFT_DOMINANT0, axis=1)\n",
    "        else:\n",
    "            data = tf.gather(data, LANDMARK_IDXS_RIGHT_DOMINANT0, axis=1)\n",
    "            data = (\n",
    "                    self.normalisation_correction + (\n",
    "                        (data - self.normalisation_correction) * tf.where(self.normalisation_correction != 0, -1.0, 1.0))\n",
    "                )\n",
    "\n",
    "        # Video fits in INPUT_SIZE\n",
    "        if N_FRAMES < INPUT_SIZE:\n",
    "            # Pad With -1 to indicate padding\n",
    "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
    "            # Pad Data With Zeros\n",
    "            data = tf.pad(data, [[0, INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            return data, non_empty_frames_idxs\n",
    "        # Video needs to be downsampled to INPUT_SIZE\n",
    "        else:\n",
    "            # Repeat\n",
    "            if N_FRAMES < INPUT_SIZE**2:\n",
    "                repeats = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, N_FRAMES0)\n",
    "                data = tf.repeat(data, repeats=repeats, axis=0)\n",
    "                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n",
    "\n",
    "            # Pad To Multiple Of Input Size\n",
    "            pool_size = tf.math.floordiv(len(data), INPUT_SIZE)\n",
    "            if tf.math.mod(len(data), INPUT_SIZE) > 0:\n",
    "                pool_size += 1\n",
    "\n",
    "            if pool_size == 1:\n",
    "                pad_size = (pool_size * INPUT_SIZE) - len(data)\n",
    "            else:\n",
    "                pad_size = (pool_size * INPUT_SIZE) % len(data)\n",
    "\n",
    "            # Pad Start/End with Start/End value\n",
    "            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            if tf.math.mod(pad_size, 2) > 0:\n",
    "                pad_right += 1\n",
    "\n",
    "            # Pad By Concatenating Left/Right Edge Values\n",
    "            data = self.pad_edge(data, pad_left, 'LEFT')\n",
    "            data = self.pad_edge(data, pad_right, 'RIGHT')\n",
    "\n",
    "            # Pad Non Empty Frame Indices\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n",
    "\n",
    "            # Reshape to Mean Pool\n",
    "            data = tf.reshape(data, [INPUT_SIZE, -1, N_COLS, N_DIMS])\n",
    "            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [INPUT_SIZE, -1])\n",
    "\n",
    "            # Mean Pool\n",
    "            data = tf.experimental.numpy.nanmean(data, axis=1)\n",
    "            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n",
    "\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "\n",
    "            return data, non_empty_frames_idxs\n",
    "\n",
    "preprocess_layer = PreprocessLayer()\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_and_preprocess_data(data, preprocess_layer):\n",
    "    # Load data\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = data[data_columns]\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    # Apply preprocessing using the PreprocessLayer\n",
    "    processed_data = preprocess_layer(data.astype(np.float32))\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference LSTM Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Camera Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\AAI-521_FinalProject_Team2\\Inference_local.ipynb Cell 18\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m NON_EMPTY_FRAME_IDXS[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m non_empty_frame_idxs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# predicted = model_transformer.predict({ 'frames': X, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS }, verbose=0).argmax(axis=1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# transformer_predicted_action = class_names[predicted[0]]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Assuming model_transformer.predict gives you probabilities for each class\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m predictions \u001b[39m=\u001b[39m model_transformer\u001b[39m.\u001b[39;49mpredict({ \u001b[39m'\u001b[39;49m\u001b[39mframes\u001b[39;49m\u001b[39m'\u001b[39;49m: X, \u001b[39m'\u001b[39;49m\u001b[39mnon_empty_frame_idxs\u001b[39;49m\u001b[39m'\u001b[39;49m: NON_EMPTY_FRAME_IDXS }, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m top_3_indices \u001b[39m=\u001b[39m predictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margsort()[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:][::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Inference_local.ipynb#X50sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m transformer_results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2221\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2222\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2223\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2224\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2225\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2226\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2227\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2228\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2229\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2230\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2231\u001b[0m )\n\u001b[0;32m   2233\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1263\u001b[0m     x,\n\u001b[0;32m   1264\u001b[0m     y,\n\u001b[0;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1274\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:308\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m indices\n\u001b[0;32m    303\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# performance.\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    311\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[39m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39m      A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2202\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2199\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m DEBUG_MODE:\n\u001b[0;32m   2200\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2201\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2202\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39;49m, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   2203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2204\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2205\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[0;32m   2206\u001b[0m       map_func,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2209\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2210\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5406\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39mStructuredFunctionWrapper(\n\u001b[0;32m   5401\u001b[0m     map_func,\n\u001b[0;32m   5402\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformation_name(),\n\u001b[0;32m   5403\u001b[0m     dataset\u001b[39m=\u001b[39minput_dataset,\n\u001b[0;32m   5404\u001b[0m     use_legacy_function\u001b[39m=\u001b[39muse_legacy_function)\n\u001b[0;32m   5405\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m-> 5406\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[0;32m   5407\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   5409\u001b[0m     f\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction,\n\u001b[0;32m   5410\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m   5411\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5412\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n\u001b[0;32m   5413\u001b[0m \u001b[39msuper\u001b[39m(MapDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3459\u001b[0m, in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   3457\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3458\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3459\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3460\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMapDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f,\n\u001b[0;32m   3461\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes,\n\u001b[0;32m   3462\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39muse_inter_op_parallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_inter_op_parallelism,\n\u001b[0;32m   3463\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mpreserve_cardinality\u001b[39;49m\u001b[39m\"\u001b[39;49m, preserve_cardinality, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[0;32m   3464\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3465\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=0)\n",
    "\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "sequence = []\n",
    "printed = False\n",
    "\n",
    "count = 0\n",
    "message = 'Loading...'\n",
    "transformer_results = []\n",
    "lstm_results = ''\n",
    "vframe_number = 0\n",
    "combine_df = pd.DataFrame()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Transformer pre processing\n",
    "    vframe_number += 1\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    testdf =  transform(results, vframe_number)\n",
    "    combine_df = pd.concat([combine_df, testdf])\n",
    "\n",
    "    # LSTM pre processing\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    keypoints = extract_keypoints(results)\n",
    "    # keypoints = np.nan_to_num(keypoints)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-10:]\n",
    "\n",
    "    if vframe_number == 24:\n",
    "\n",
    "        if not printed:\n",
    "          print('predicting...')\n",
    "          printed = True\n",
    "\n",
    "        # LSTM Prediction\n",
    "        # predictions = model_lstm.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "        # max_confidence = np.max(predictions)\n",
    "        # lstm_predicted_action = actions[np.argmax(predictions)]\n",
    "        # lstm_results = 'LSTM: ' + lstm_predicted_action\n",
    "\n",
    "        # Transformer Inference\n",
    "        processed_data, non_empty_frame_idxs = load_and_preprocess_data(combine_df, preprocess_layer)\n",
    "        X = np.zeros([1, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n",
    "        NON_EMPTY_FRAME_IDXS = np.full([1, INPUT_SIZE], -1, dtype=np.float32)\n",
    "        X[0] = processed_data\n",
    "        NON_EMPTY_FRAME_IDXS[0] = non_empty_frame_idxs\n",
    "        predictions = model_transformer.predict({ 'frames': X, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS }, verbose=0)\n",
    "        top_3_indices = predictions[0].argsort()[-3:][::-1]\n",
    "        transformer_results = []\n",
    "        for i in top_3_indices:\n",
    "            transformer_results.append(f\"{class_names[i]}, Confidence: {predictions[0][i]*100:.2f}%\")\n",
    "\n",
    "        combine_df = pd.DataFrame()\n",
    "        vframe_number = 0\n",
    "        message = f\"Predicted Actions:\"\n",
    "\n",
    "\n",
    "    draw_styled_landmarks(image, results)\n",
    "    \n",
    "    # Add text to the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = .8\n",
    "    font_color = (255, 255, 255)\n",
    "    line_type = 2\n",
    "    position = (50, 50) \n",
    "    cv2.putText(image, message, position, font, font_scale, font_color, line_type)\n",
    "    offset = 100\n",
    "    for result in transformer_results:\n",
    "        position = (50, offset)\n",
    "        cv2.putText(image, result, position, font, font_scale, font_color, line_type)\n",
    "        offset += 50\n",
    "    # if (lstm_results != ''):\n",
    "    #     position = (50, 150) \n",
    "    #     cv2.putText(image, lstm_results, position, font, font_scale, font_color, line_type)\n",
    "\n",
    "    _, jpeg_image = cv2.imencode('.jpg', image)\n",
    "    i = Image(data=jpeg_image.tobytes())\n",
    "    display(i)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
