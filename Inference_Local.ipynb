{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\paula\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from IPython.display import display, Javascript, Image\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_dir = './Transfomer/model/'\n",
    "lstm_dir = './LSTM/model-top-15/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "\n",
    "During our study of the data and research on the possible model solutions, there is one transformer model approach caught our eye. This transformer model approach was designed by Wijkhuizen, M., in the Kaggle competition (2023). Our project team decided to follow Wijkhuizen, M.’s approach to create a transformer model as one of the models to test for this project. Our goal with this approach is to get a better understanding of the transformer model since Wijkhuizen, M.’s approach is to build a transformer model from scratch and not fine-turn a base model.\n",
    "\n",
    "The attention mechanism is a key component in Transformer models, enabling the model to focus on different parts of the input sequence for each step of the output sequence. Attention enables the model to concentrate selectively on various segments of the input sequence for making predictions, rather than interpreting the entire sequence as a uniform-length vector. This feature has been crucial in the triumph of the transformer model, sparking extensive subsequent research and the development of numerous new models (Kumar, A. 2023). Wijkhuizen, M.’s  custom transformer model deployed attention_mask in the Scaled Dot-Product function in a different way compared to the classic transformer model in that this mask is applied in the Softmax step to selectively ignore or pay less attention to certain parts of the input, such as padding or irrelevant frames in a video sequence. Also, the Softmax layer was used instead of the Softmax function. The attention mechanism allows the model to focus on different parts of the input sequence dynamically, which is crucial for tasks like ASL recognition. In ASL, the importance of different landmarks can vary significantly across different signs. The multi-head attention mechanism is particularly well-suited to capture these varied dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HAND_IDXS: 21, N_COLS: 66\n",
      "LIPS_START: 0, LEFT_HAND_START: 40, RIGHT_HAND_START: 61, POSE_START: 61\n"
     ]
    }
   ],
   "source": [
    "# Code From https://www.kaggle.com/code/markwijkhuizen/gislr-tf-data-processing-transformer-training\n",
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# Dense layer units for landmarks\n",
    "LIPS_UNITS = 384\n",
    "HANDS_UNITS = 384\n",
    "POSE_UNITS = 384\n",
    "# final embedding and transformer embedding size\n",
    "UNITS = 512\n",
    "\n",
    "# Transformer\n",
    "NUM_BLOCKS = 2\n",
    "MLP_RATIO = 2\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.00\n",
    "MLP_DROPOUT_RATIO = 0.30\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
    "\n",
    "# Initiailizers\n",
    "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "GELU = tf.keras.activations.gelu\n",
    "\n",
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "# True: use 10% of participants as validation set\n",
    "# False: use all data for training -> gives better LB result\n",
    "USE_VAL = False\n",
    "\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "IS_INTERACTIVE = True\n",
    "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
    "\n",
    "INPUT_SIZE = 64\n",
    "\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 100\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237\n",
    "\n",
    "USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
    "START_IDX = 468\n",
    "LIPS_IDXS0 = np.array([\n",
    "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "    ])\n",
    "# Landmark indices in original data\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
    "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
    "LANDMARK_IDXS_LEFT_DOMINANT0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, LEFT_POSE_IDXS0))\n",
    "LANDMARK_IDXS_RIGHT_DOMINANT0 = np.concatenate((LIPS_IDXS0, RIGHT_HAND_IDXS0, RIGHT_POSE_IDXS0))\n",
    "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
    "N_COLS = LANDMARK_IDXS_LEFT_DOMINANT0.size\n",
    "# Landmark indices in processed data\n",
    "LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LIPS_IDXS0)).squeeze()\n",
    "LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_HAND_IDXS0)).squeeze()\n",
    "RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, RIGHT_HAND_IDXS0)).squeeze()\n",
    "HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, HAND_IDXS0)).squeeze()\n",
    "POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_POSE_IDXS0)).squeeze()\n",
    "\n",
    "print(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')\n",
    "\n",
    "LIPS_START = 0\n",
    "LEFT_HAND_START = LIPS_IDXS.size\n",
    "RIGHT_HAND_START = LEFT_HAND_START + LEFT_HAND_IDXS.size\n",
    "POSE_START = RIGHT_HAND_START + RIGHT_HAND_IDXS.size\n",
    "\n",
    "print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')\n",
    "\n",
    "LIPS_MEAN = np.load(f'{transformer_dir}/LIPS_MEAN.npy')\n",
    "LIPS_STD = np.load(f'{transformer_dir}/LIPS_STD.npy')\n",
    "LEFT_HANDS_MEAN = np.load(f'{transformer_dir}/LEFT_HANDS_MEAN.npy')\n",
    "LEFT_HANDS_STD = np.load(f'{transformer_dir}/LEFT_HANDS_STD.npy')\n",
    "POSE_MEAN = np.load(f'{transformer_dir}/POSE_MEAN.npy')\n",
    "POSE_STD = np.load(f'{transformer_dir}/POSE_STD.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code From https://www.kaggle.com/code/markwijkhuizen/gislr-tf-data-processing-transformer-training\n",
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "    def get_diffs(self, l):\n",
    "        S = l.shape[2]\n",
    "        other = tf.expand_dims(l, 3)\n",
    "        other = tf.repeat(other, S, axis=3)\n",
    "        other = tf.transpose(other, [0,1,3,2])\n",
    "        diffs = tf.expand_dims(l, 3) - other\n",
    "        diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "        return diffs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Positional Embedding, initialized with zeros\n",
    "        self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "        # Embedding layer for Landmarks\n",
    "        self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
    "        self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
    "        self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
    "        # Landmark Weights\n",
    "        self.landmark_weights = tf.Variable(tf.zeros([3], dtype=tf.float32), name='landmark_weights')\n",
    "        # Fully Connected Layers for combined landmarks\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "            tf.keras.layers.Activation(GELU),\n",
    "            tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name='fc')\n",
    "\n",
    "\n",
    "    def call(self, lips0, left_hand0, pose0, non_empty_frame_idxs, training=False):\n",
    "        # Lips\n",
    "        lips_embedding = self.lips_embedding(lips0)\n",
    "        # Left Hand\n",
    "        left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "        # Pose\n",
    "        pose_embedding = self.pose_embedding(pose0)\n",
    "        # Merge Embeddings of all landmarks with mean pooling\n",
    "        x = tf.stack((\n",
    "            lips_embedding, left_hand_embedding, pose_embedding,\n",
    "        ), axis=3)\n",
    "        x = x * tf.nn.softmax(self.landmark_weights)\n",
    "        x = tf.reduce_sum(x, axis=3)\n",
    "        # Fully Connected Layers\n",
    "        x = self.fc(x)\n",
    "        # Add Positional Embedding\n",
    "        max_frame_idxs = tf.clip_by_value(\n",
    "                tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True),\n",
    "                1,\n",
    "                np.PINF,\n",
    "            )\n",
    "        normalised_non_empty_frame_idxs = tf.where(\n",
    "            tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "            INPUT_SIZE,\n",
    "            tf.cast(\n",
    "                non_empty_frame_idxs / max_frame_idxs * INPUT_SIZE,\n",
    "                tf.int32,\n",
    "            ),\n",
    "        )\n",
    "        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LandmarkEmbedding(tf.keras.Model):\n",
    "    def __init__(self, units, name):\n",
    "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Embedding for missing landmark in frame, initizlied with zeros\n",
    "        self.empty_embedding = self.add_weight(\n",
    "            name=f'{self.name}_empty_embedding',\n",
    "            shape=[self.units],\n",
    "            initializer=INIT_ZEROS,\n",
    "        )\n",
    "        # Embedding\n",
    "        self.dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "            tf.keras.layers.Activation(GELU),\n",
    "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "        ], name=f'{self.name}_dense')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.where(\n",
    "                # Checks whether landmark is missing in frame\n",
    "                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                # If so, the empty embedding is used\n",
    "                self.empty_embedding,\n",
    "                # Otherwise the landmark data is embedded\n",
    "                self.dense(x),\n",
    "            )\n",
    "\n",
    "# Full Transformer\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_blocks):\n",
    "        super(Transformer, self).__init__(name='transformer')\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.ln_1s = []\n",
    "        self.mhas = []\n",
    "        self.ln_2s = []\n",
    "        self.mlps = []\n",
    "        # Make Transformer Blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            # Multi Head Attention\n",
    "            self.mhas.append(MultiHeadAttention(UNITS, 8))\n",
    "            # Multi Layer Perception\n",
    "            self.mlps.append(tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(UNITS * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
    "                tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ]))\n",
    "\n",
    "    def call(self, x, attention_mask):\n",
    "        # Iterate input over transformer blocks\n",
    "        for mha, mlp in zip(self.mhas, self.mlps):\n",
    "            x = x + mha(x, attention_mask)\n",
    "            x = x + mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
    "# replaced softmax with softmax layer to support masked softmax\n",
    "def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "    #calculates Q . K(transpose)\n",
    "    qkt = tf.matmul(q,k,transpose_b=True)\n",
    "    #caculates scaling factor\n",
    "    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "    scaled_qkt = qkt/dk\n",
    "    softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "\n",
    "    z = tf.matmul(softmax,v)\n",
    "    #shape: (m,Tx,depth), same shape as q,k,v\n",
    "    return z\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,num_of_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.depth = d_model//num_of_heads\n",
    "        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self,x, attention_mask):\n",
    "\n",
    "        multi_attn = []\n",
    "        for i in range(self.num_of_heads):\n",
    "            Q = self.wq[i](x)\n",
    "            K = self.wk[i](x)\n",
    "            V = self.wv[i](x)\n",
    "            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "\n",
    "        multi_head = tf.concat(multi_attn,axis=-1)\n",
    "        multi_head_attention = self.wo(multi_head)\n",
    "        return multi_head_attention\n",
    "\n",
    "# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\n",
    "def scce_with_ls(y_true, y_pred):\n",
    "    # One Hot Encode Sparsely Encoded Target Sign\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "    y_true = tf.one_hot(y_true, NUM_CLASSES, axis=1)\n",
    "    y_true = tf.squeeze(y_true, axis=2)\n",
    "    # Categorical Crossentropy with native label smoothing support\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25)\n",
    "\n",
    "def get_transformer_model():\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask0 = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask0 = tf.expand_dims(mask0, axis=2)\n",
    "    # Random Frame Masking\n",
    "    mask = tf.where(\n",
    "        (tf.random.uniform(tf.shape(mask0)) > 0.25) & tf.math.not_equal(mask0, 0.0),\n",
    "        1.0,\n",
    "        0.0,\n",
    "    )\n",
    "    # Correct Samples Which are all masked now...\n",
    "    mask = tf.where(\n",
    "        tf.math.equal(tf.reduce_sum(mask, axis=[1,2], keepdims=True), 0.0),\n",
    "        mask0,\n",
    "        mask,\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    # LIPS\n",
    "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 40, 2])\n",
    "    lips = tf.where(\n",
    "            tf.math.equal(lips, 0.0),\n",
    "            0.0,\n",
    "            (lips - LIPS_MEAN) / LIPS_STD,\n",
    "        )\n",
    "    # LEFT HAND\n",
    "    left_hand = tf.slice(x, [0,0,40,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    left_hand = tf.where(\n",
    "            tf.math.equal(left_hand, 0.0),\n",
    "            0.0,\n",
    "            (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "        )\n",
    "    # POSE\n",
    "    pose = tf.slice(x, [0,0,61,0], [-1,INPUT_SIZE, 5, 2])\n",
    "    pose = tf.where(\n",
    "            tf.math.equal(pose, 0.0),\n",
    "            0.0,\n",
    "            (pose - POSE_MEAN) / POSE_STD,\n",
    "        )\n",
    "\n",
    "    # Flatten\n",
    "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 40*2])\n",
    "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 5*2])\n",
    "\n",
    "    # Embedding\n",
    "    x = Embedding()(lips, left_hand, pose, non_empty_frame_idxs)\n",
    "\n",
    "    # Encoder Transformer Blocks\n",
    "    x = Transformer(NUM_BLOCKS)(x, mask)\n",
    "\n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classifier Dropout\n",
    "    x = tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "\n",
    "    outputs = x\n",
    "\n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "\n",
    "    # Sparse Categorical Cross Entropy With Label Smoothing\n",
    "    loss = scce_with_ls\n",
    "\n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_transformer = get_transformer_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.load_weights(f'{transformer_dir}/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "signmap_sub_dir = 'sign_to_prediction_index_map.json'\n",
    "signmap_full_file_path = os.path.join(transformer_dir, signmap_sub_dir)\n",
    "# Load the sign to index mapping\n",
    "with open(signmap_full_file_path, 'r') as file:\n",
    "    sign_to_index = json.load(file)\n",
    "\n",
    "inverted_mapping = {v: k for k, v in sign_to_index.items()}\n",
    "# Convert mapping to list\n",
    "class_names = [inverted_mapping[i] for i in sorted(inverted_mapping)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required files not found in the folder.\n"
     ]
    }
   ],
   "source": [
    "load_lstm = False\n",
    "h5_file = None\n",
    "npy_file = None\n",
    "\n",
    "for file in os.listdir(lstm_dir):\n",
    "    if file.endswith('.h5') and not h5_file:\n",
    "        h5_file = file\n",
    "    elif file.endswith('.npy') and not npy_file:\n",
    "        npy_file = file\n",
    "    if h5_file and npy_file:\n",
    "        break\n",
    "\n",
    "if h5_file and npy_file and load_lstm:\n",
    "    actions = np.load(os.path.join(lstm_dir, npy_file), allow_pickle=True)\n",
    "\n",
    "    # model = load_model(os.path.join(lstm_dir, h5_file))\n",
    "    num_classes = actions.shape[0]\n",
    "    model_lstm = Sequential()\n",
    "    model_lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(10, 1662)))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(BatchNormalization())\n",
    "    model_lstm.add(LSTM(256, return_sequences=False, activation='relu'))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(128, activation='relu'))\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model_lstm.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model_lstm.load_weights(os.path.join(lstm_dir, h5_file))\n",
    "    print(\"Model and actions loaded successfully.\")\n",
    "else:\n",
    "    print(\"Required files not found in the folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Transformer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = []\n",
    "TenDataFrame = []\n",
    "\n",
    "def transform(results, frame_number):\n",
    "  frame = []\n",
    "  type_ = []\n",
    "  index = []\n",
    "  x = []\n",
    "  y = []\n",
    "  z = []\n",
    "  #image.flags.writeable = False\n",
    "  #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  #results = holistic.process(image)\n",
    "  #face\n",
    "  if(results.face_landmarks is None):\n",
    "    for ind in range(468):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"face\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.face_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"face\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #left hand\n",
    "  if(results.left_hand_landmarks is None):\n",
    "    for ind in range(21):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"left_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.left_hand_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"left_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #pose\n",
    "  if(results.pose_landmarks is None):\n",
    "    for ind in range(33):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"pose\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.pose_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"pose\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "\n",
    "  #right hand\n",
    "  if(results.right_hand_landmarks is None):\n",
    "    for ind in range(21):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"right_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(np.nan)\n",
    "      y.append(np.nan)\n",
    "      z.append(np.nan)\n",
    "  else:\n",
    "    for ind,val in enumerate(results.right_hand_landmarks.landmark):\n",
    "      frame.append(frame_number)\n",
    "      type_.append(\"right_hand\")\n",
    "      index.append(ind)\n",
    "      x.append(val.x)\n",
    "      y.append(val.y)\n",
    "      z.append(val.z)\n",
    "  #data = np.array([frame, type_, index, x, y, z])\n",
    "  return pd.DataFrame({\"frame\" : frame,\"type\"  : type_,\"landmark_index\" : index,\"x\" : x,\"y\" : y,\"z\" : z})\n",
    "\n",
    "# Code From https://www.kaggle.com/code/markwijkhuizen/gislr-tf-data-processing-transformer-training\n",
    "\"\"\"\n",
    "    Tensorflow layer to process data in TFLite\n",
    "    Data needs to be processed in the model itself, so we can not use Python\n",
    "\"\"\"\n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PreprocessLayer, self).__init__()\n",
    "        normalisation_correction = tf.constant([\n",
    "                    # Add 0.50 to left hand (original right hand) and substract 0.50 of right hand (original left hand)\n",
    "                    [0] * len(LIPS_IDXS) + [0.50] * len(LEFT_HAND_IDXS) + [0.50] * len(POSE_IDXS),\n",
    "                    # Y coordinates stay intact\n",
    "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
    "                    # Z coordinates stay intact\n",
    "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "        self.normalisation_correction = tf.transpose(normalisation_correction, [1,0])\n",
    "\n",
    "    def pad_edge(self, t, repeats, side):\n",
    "        if side == 'LEFT':\n",
    "            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n",
    "        elif side == 'RIGHT':\n",
    "            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None,N_ROWS,N_DIMS], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, data0):\n",
    "        # Number of Frames in Video\n",
    "        N_FRAMES0 = tf.shape(data0)[0]\n",
    "\n",
    "        # Find dominant hand by comparing summed absolute coordinates\n",
    "        left_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1))\n",
    "        right_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1))\n",
    "        left_dominant = left_hand_sum >= right_hand_sum\n",
    "\n",
    "        # Count non NaN Hand values in each frame for the dominant hand\n",
    "        if left_dominant:\n",
    "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
    "                    tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1),\n",
    "                    axis=[1, 2],\n",
    "                )\n",
    "        else:\n",
    "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
    "                    tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1),\n",
    "                    axis=[1, 2],\n",
    "                )\n",
    "\n",
    "        # Find frames indices with coordinates of dominant hand\n",
    "        non_empty_frames_idxs = tf.where(frames_hands_non_nan_sum > 0)\n",
    "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
    "        # Filter frames\n",
    "        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n",
    "\n",
    "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
    "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32)\n",
    "        # Normalize to start with 0\n",
    "        non_empty_frames_idxs -= tf.reduce_min(non_empty_frames_idxs)\n",
    "\n",
    "        # Number of Frames in Filtered Video\n",
    "        N_FRAMES = tf.shape(data)[0]\n",
    "\n",
    "        # Gather Relevant Landmark Columns\n",
    "        if left_dominant:\n",
    "            data = tf.gather(data, LANDMARK_IDXS_LEFT_DOMINANT0, axis=1)\n",
    "        else:\n",
    "            data = tf.gather(data, LANDMARK_IDXS_RIGHT_DOMINANT0, axis=1)\n",
    "            data = (\n",
    "                    self.normalisation_correction + (\n",
    "                        (data - self.normalisation_correction) * tf.where(self.normalisation_correction != 0, -1.0, 1.0))\n",
    "                )\n",
    "\n",
    "        # Video fits in INPUT_SIZE\n",
    "        if N_FRAMES < INPUT_SIZE:\n",
    "            # Pad With -1 to indicate padding\n",
    "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
    "            # Pad Data With Zeros\n",
    "            data = tf.pad(data, [[0, INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            return data, non_empty_frames_idxs\n",
    "        # Video needs to be downsampled to INPUT_SIZE\n",
    "        else:\n",
    "            # Repeat\n",
    "            if N_FRAMES < INPUT_SIZE**2:\n",
    "                repeats = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, N_FRAMES0)\n",
    "                data = tf.repeat(data, repeats=repeats, axis=0)\n",
    "                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n",
    "\n",
    "            # Pad To Multiple Of Input Size\n",
    "            pool_size = tf.math.floordiv(len(data), INPUT_SIZE)\n",
    "            if tf.math.mod(len(data), INPUT_SIZE) > 0:\n",
    "                pool_size += 1\n",
    "\n",
    "            if pool_size == 1:\n",
    "                pad_size = (pool_size * INPUT_SIZE) - len(data)\n",
    "            else:\n",
    "                pad_size = (pool_size * INPUT_SIZE) % len(data)\n",
    "\n",
    "            # Pad Start/End with Start/End value\n",
    "            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            if tf.math.mod(pad_size, 2) > 0:\n",
    "                pad_right += 1\n",
    "\n",
    "            # Pad By Concatenating Left/Right Edge Values\n",
    "            data = self.pad_edge(data, pad_left, 'LEFT')\n",
    "            data = self.pad_edge(data, pad_right, 'RIGHT')\n",
    "\n",
    "            # Pad Non Empty Frame Indices\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n",
    "\n",
    "            # Reshape to Mean Pool\n",
    "            data = tf.reshape(data, [INPUT_SIZE, -1, N_COLS, N_DIMS])\n",
    "            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [INPUT_SIZE, -1])\n",
    "\n",
    "            # Mean Pool\n",
    "            data = tf.experimental.numpy.nanmean(data, axis=1)\n",
    "            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n",
    "\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "\n",
    "            return data, non_empty_frames_idxs\n",
    "\n",
    "preprocess_layer = PreprocessLayer()\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_and_preprocess_data(data, preprocess_layer):\n",
    "    # Load data\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = data[data_columns]\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    # Apply preprocessing using the PreprocessLayer\n",
    "    processed_data = preprocess_layer(data.astype(np.float32))\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference LSTM Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Camera Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=0)\n",
    "\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "sequence = []\n",
    "printed = False\n",
    "\n",
    "count = 0\n",
    "message = 'Loading...'\n",
    "transformer_results = []\n",
    "lstm_results = ''\n",
    "vframe_number = 0\n",
    "combine_df = pd.DataFrame()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Transformer pre processing\n",
    "    vframe_number += 1\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    testdf =  transform(results, vframe_number)\n",
    "    combine_df = pd.concat([combine_df, testdf])\n",
    "\n",
    "    # LSTM pre processing\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    keypoints = extract_keypoints(results)\n",
    "    # keypoints = np.nan_to_num(keypoints)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-10:]\n",
    "\n",
    "    if vframe_number == 24:\n",
    "\n",
    "        if not printed:\n",
    "          print('predicting...')\n",
    "          printed = True\n",
    "\n",
    "        # LSTM Prediction\n",
    "        # predictions = model_lstm.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "        # max_confidence = np.max(predictions)\n",
    "        # lstm_predicted_action = actions[np.argmax(predictions)]\n",
    "        # lstm_results = 'LSTM: ' + lstm_predicted_action\n",
    "\n",
    "        # Transformer Inference\n",
    "        processed_data, non_empty_frame_idxs = load_and_preprocess_data(combine_df, preprocess_layer)\n",
    "        X = np.zeros([1, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n",
    "        NON_EMPTY_FRAME_IDXS = np.full([1, INPUT_SIZE], -1, dtype=np.float32)\n",
    "        X[0] = processed_data\n",
    "        NON_EMPTY_FRAME_IDXS[0] = non_empty_frame_idxs\n",
    "        predictions = model_transformer.predict({ 'frames': X, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS }, verbose=0)\n",
    "        top_3_indices = predictions[0].argsort()[-3:][::-1]\n",
    "        transformer_results = []\n",
    "        for i in top_3_indices:\n",
    "            transformer_results.append(f\"{class_names[i]}, Confidence: {predictions[0][i]*100:.2f}%\")\n",
    "\n",
    "        combine_df = pd.DataFrame()\n",
    "        vframe_number = 0\n",
    "        message = f\"Predicted Actions:\"\n",
    "\n",
    "\n",
    "    draw_styled_landmarks(image, results)\n",
    "    \n",
    "    # Add text to the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = .8\n",
    "    font_color = (255, 255, 255)\n",
    "    line_type = 2\n",
    "    position = (50, 50) \n",
    "    cv2.putText(image, message, position, font, font_scale, font_color, line_type)\n",
    "    offset = 100\n",
    "    for result in transformer_results:\n",
    "        position = (50, offset)\n",
    "        cv2.putText(image, result, position, font, font_scale, font_color, line_type)\n",
    "        offset += 50\n",
    "    # if (lstm_results != ''):\n",
    "    #     position = (50, 150) \n",
    "    #     cv2.putText(image, lstm_results, position, font, font_scale, font_color, line_type)\n",
    "\n",
    "    _, jpeg_image = cv2.imencode('.jpg', image)\n",
    "    i = Image(data=jpeg_image.tobytes())\n",
    "    display(i)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo Signs:\n",
    "- Owl\n",
    "- Bug\n",
    "- Where\n",
    "- Hungry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
