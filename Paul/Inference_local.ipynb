{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from IPython.display import display, Javascript, Image\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model and actions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "folder_path = './model-top-15'\n",
    "\n",
    "h5_file = None\n",
    "npy_file = None\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.h5') and not h5_file:\n",
    "        h5_file = file\n",
    "    elif file.endswith('.npy') and not npy_file:\n",
    "        npy_file = file\n",
    "    if h5_file and npy_file:\n",
    "        break\n",
    "\n",
    "if h5_file and npy_file:\n",
    "    actions = np.load(os.path.join(folder_path, npy_file), allow_pickle=True)\n",
    "\n",
    "    # model = load_model(os.path.join(folder_path, h5_file))\n",
    "    num_classes = actions.shape[0]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(10, 1662)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(256, return_sequences=False, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.load_weights(os.path.join(folder_path, h5_file))\n",
    "    print(\"Model and actions loaded successfully.\")\n",
    "else:\n",
    "    print(\"Required files not found in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\AAI-521_FinalProject_Team2\\Paul\\Inference_local.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Paul/Inference_local.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Paul/Inference_local.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Break the loop if 'q' is pressed\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Paul/Inference_local.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m10\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/AAI-521_FinalProject_Team2/Paul/Inference_local.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=0)\n",
    "\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "sequence = []\n",
    "left_hand_located = []\n",
    "right_hand_located = []\n",
    "printed = False\n",
    "\n",
    "count = 0\n",
    "message = 'Loading...'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # ... [other code]\n",
    "    image, results = mediapipe_detection(frame, holistic)\n",
    "    left_hand_located.append(bool(results.left_hand_landmarks))\n",
    "    left_hand_located = left_hand_located[-10:]\n",
    "    right_hand_located.append(bool(results.right_hand_landmarks))\n",
    "    right_hand_located = right_hand_located[-10:]\n",
    "    all_left_hands_detected = all(left_hand_located)\n",
    "    all_right_hands_detected = all(right_hand_located)\n",
    "    keypoints = extract_keypoints(results)\n",
    "    # keypoints = np.nan_to_num(keypoints)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-10:]\n",
    "\n",
    "    if len(sequence) == 10:\n",
    "        if (not (all_left_hands_detected or all_right_hands_detected)):\n",
    "          message = \"Not enough hand data\"\n",
    "        else:\n",
    "            infer = np.nan_to_num(sequence)\n",
    "            count += 1\n",
    "            if (count > 5):\n",
    "                predictions  = model.predict(np.expand_dims(infer, axis=0), verbose=0)[0]\n",
    "                top_indices = predictions.argsort()[-3:][::-1]  # Indices of top 3 predictions\n",
    "                top_predictions = [(actions[i], predictions[i]) for i in top_indices]  # (action, confidence)\n",
    "\n",
    "                message = ''\n",
    "                for action, confidence in top_predictions:\n",
    "                    message += f'{action}: {confidence:.2f} | '  # Format the message\n",
    "                count = 0\n",
    "\n",
    "    draw_styled_landmarks(image, results)\n",
    "    \n",
    "    # Add text to the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_color = (255, 255, 255)\n",
    "    line_type = 2\n",
    "    position = (50, 50) \n",
    "    cv2.putText(image, message, position, font, font_scale, font_color, line_type)\n",
    "\n",
    "    _, jpeg_image = cv2.imencode('.jpg', image)\n",
    "    i = Image(data=jpeg_image.tobytes())\n",
    "    display(i)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
