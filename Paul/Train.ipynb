{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../Dataset_GISLR/asl-signs/'\n",
    "PROCESSED_OUTPUT_PATH = './Dataset_GISLR_Processed/'\n",
    "MODEL_VERSION = 'high-perform-test'\n",
    "save_low_performers = True\n",
    "save_high_performers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 08:37:44.380617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-02 08:37:44.380665: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-02 08:37:44.380683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-02 08:37:44.385081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Labels from the PreProcessed Data. Optionally filter out labels to remove any identified low performing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n",
      "53\n",
      "['aunt' 'bird' 'black' 'brother' 'brown' 'bug' 'callonphone' 'cheek'\n",
      " 'clown' 'cow' 'cute' 'dad' 'doll' 'donkey' 'drink' 'ear' 'eye' 'feet'\n",
      " 'find' 'fireman' 'flower' 'for' 'frog' 'grandpa' 'grass' 'gum' 'hair'\n",
      " 'hen' 'home' 'horse' 'lamp' 'mad' 'mom' 'mouse' 'nose' 'owl' 'pig'\n",
      " 'police' 'radio' 'see' 'shhh' 'shirt' 'sick' 'stairs' 'stuck' 'taste'\n",
      " 'thirsty' 'tiger' 'uncle' 'water' 'who' 'yucky' 'zebra']\n"
     ]
    }
   ],
   "source": [
    "signs_to_train = [name for name in os.listdir(PROCESSED_OUTPUT_PATH) if os.path.isdir(os.path.join(PROCESSED_OUTPUT_PATH, name))]\n",
    "\n",
    "low_performing_labels_file_path = 'low_performing_labels.npy'\n",
    "low_performing_labels = np.array([])\n",
    "if os.path.exists(low_performing_labels_file_path):\n",
    "    low_performing_labels = np.load(low_performing_labels_file_path, allow_pickle=True)\n",
    "print(len(low_performing_labels))\n",
    "\n",
    "high_performing_labels_file_path = 'high_performing_labels.npy'\n",
    "high_performing_labels = np.array([])\n",
    "if os.path.exists(high_performing_labels_file_path):\n",
    "    high_performing_labels = np.load(high_performing_labels_file_path, allow_pickle=True)\n",
    "print(len(high_performing_labels))\n",
    "\n",
    "# remove low performing labels from signs_to_train\n",
    "signs_to_train = [x for x in signs_to_train if x not in low_performing_labels]\n",
    "print(len(signs_to_train))\n",
    "\n",
    "# remove all non high performers\n",
    "if (len(high_performing_labels) > 0):\n",
    "    signs_to_train = [x for x in signs_to_train if x in high_performing_labels]\n",
    "    print(len(signs_to_train))\n",
    "\n",
    "signs_to_train.sort()\n",
    "actions = np.array(signs_to_train)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of the pre processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "for label_folder in os.listdir(PROCESSED_OUTPUT_PATH):\n",
    "    label = label_folder  # Use the folder name as the label\n",
    "    if (label not in signs_to_train):\n",
    "        continue\n",
    "    \n",
    "    if (label in low_performing_labels):\n",
    "        print(\"Skipping label: \", label)\n",
    "        continue\n",
    "\n",
    "    label_folder_path = os.path.join(PROCESSED_OUTPUT_PATH, label_folder)\n",
    "\n",
    "    # Iterate through the files in the label folder (each file is a sequence)\n",
    "    for sequence_file in os.listdir(label_folder_path):\n",
    "        if sequence_file.endswith('.npy'):\n",
    "            # Load the sequence from the file\n",
    "            sequence = np.load(os.path.join(label_folder_path, sequence_file))\n",
    "            \n",
    "            # Append the sequence and label to the respective lists\n",
    "            sequences.append(sequence)\n",
    "            labels.append(label_map[label])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13288, 10, 1662)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13288, 53)\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(labels).astype(int)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train split all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into training and a combined validation/test set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)  # Adjust the test_size as needed\n",
    "\n",
    "# Now, split the combined validation/test set into separate validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)  # This will split the remaining 30% into two 15% sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1994, 53)\n",
      "(1993, 53)\n",
      "(9301, 53)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all nan else the model will not train correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train: True, NaNs in y_train: False\n",
      "NaNs in X_val: True, NaNs in y_val: False\n",
      "NaNs in X_test: True, NaNs in y_test: False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the datasets\n",
    "print(f\"NaNs in X_train: {np.isnan(X_train).any()}, NaNs in y_train: {np.isnan(y_train).any()}\")\n",
    "print(f\"NaNs in X_val: {np.isnan(X_val).any()}, NaNs in y_val: {np.isnan(y_val).any()}\")\n",
    "print(f\"NaNs in X_test: {np.isnan(X_test).any()}, NaNs in y_test: {np.isnan(y_test).any()}\")\n",
    "\n",
    "# Replace NaN values in the datasets\n",
    "X_train = np.nan_to_num(X_train)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "X_val = np.nan_to_num(X_val)\n",
    "y_val = np.nan_to_num(y_val)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "y_test = np.nan_to_num(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 08:37:54.430084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.432988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.433036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.434702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.434736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.434757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.719876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.719921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.719929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-02 08:37:54.719956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-02 08:37:54.719971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21472 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-12-02 08:37:55.285017: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# v1\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(10,1662)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "frame_length = 1662\n",
    "num_classes = actions.shape[0]\n",
    "\n",
    "# v2\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(10, frame_length)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(256, return_sequences=False, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Optimizer with Gradient Clipping\n",
    "# optimizer = Adam(learning_rate=0.0001, clipvalue=0.5)  # Adjust learning rate and clipvalue as needed\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25, verbose=1, mode='min', min_lr=0.00001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4090, compute capability 8.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 08:37:55.529626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 08:38:00.429825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb908296f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-02 08:38:00.429869: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-12-02 08:38:00.434596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-02 08:38:00.449839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-12-02 08:38:00.518010: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 167ms/step - loss: 3.9674 - accuracy: 0.0229 - val_loss: 3.9901 - val_accuracy: 0.0311 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3.8960 - accuracy: 0.0342 - val_loss: 3.9989 - val_accuracy: 0.0216 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 3.7596 - accuracy: 0.0483 - val_loss: 4.0058 - val_accuracy: 0.0211 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3.6526 - accuracy: 0.0654 - val_loss: 3.9937 - val_accuracy: 0.0321 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 3.5115 - accuracy: 0.0840 - val_loss: 3.7521 - val_accuracy: 0.0396 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 3.3786 - accuracy: 0.1028 - val_loss: 3.7308 - val_accuracy: 0.0447 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 3.2247 - accuracy: 0.1229 - val_loss: 3.6293 - val_accuracy: 0.0542 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 3.0981 - accuracy: 0.1404 - val_loss: 3.5763 - val_accuracy: 0.0833 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 3.0148 - accuracy: 0.1584 - val_loss: 3.5610 - val_accuracy: 0.0758 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.8824 - accuracy: 0.1866 - val_loss: 3.4611 - val_accuracy: 0.0682 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 2.7910 - accuracy: 0.2046 - val_loss: 3.6399 - val_accuracy: 0.0702 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.7474 - accuracy: 0.2120 - val_loss: 3.4201 - val_accuracy: 0.0838 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.5985 - accuracy: 0.2437 - val_loss: 4.9064 - val_accuracy: 0.0592 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.5362 - accuracy: 0.2666 - val_loss: 3.5530 - val_accuracy: 0.0828 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 2.4691 - accuracy: 0.2720 - val_loss: 3.1892 - val_accuracy: 0.1445 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 2.4212 - accuracy: 0.2949 - val_loss: 3.5145 - val_accuracy: 0.0948 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 2.3630 - accuracy: 0.3104 - val_loss: 4.0417 - val_accuracy: 0.0462 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.2812 - accuracy: 0.3257 - val_loss: 3.4919 - val_accuracy: 0.0973 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.2174 - accuracy: 0.3410 - val_loss: 3.4911 - val_accuracy: 0.0768 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 2.1749 - accuracy: 0.3528 - val_loss: 4.7066 - val_accuracy: 0.0622 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2.1075 - accuracy: 0.3792 - val_loss: 3.6192 - val_accuracy: 0.0973 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 2.0575 - accuracy: 0.3858 - val_loss: 4.0382 - val_accuracy: 0.0853 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2.0182 - accuracy: 0.3997 - val_loss: 3.4984 - val_accuracy: 0.1209 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 2.0045 - accuracy: 0.4036 - val_loss: 3.6899 - val_accuracy: 0.0993 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.9358 - accuracy: 0.4255 - val_loss: 4.0941 - val_accuracy: 0.0768 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.9074 - accuracy: 0.4374 - val_loss: 6.8642 - val_accuracy: 0.0336 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.9023 - accuracy: 0.4295 - val_loss: 4.4013 - val_accuracy: 0.0587 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.8373 - accuracy: 0.4539 - val_loss: 3.5164 - val_accuracy: 0.0848 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.8272 - accuracy: 0.4513 - val_loss: 5.1641 - val_accuracy: 0.0467 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.8552 - accuracy: 0.4467 - val_loss: 3.1058 - val_accuracy: 0.1681 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.7859 - accuracy: 0.4711 - val_loss: 3.5024 - val_accuracy: 0.1706 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.7010 - accuracy: 0.4883 - val_loss: 12.4740 - val_accuracy: 0.0426 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.7424 - accuracy: 0.4862 - val_loss: 6.0236 - val_accuracy: 0.0748 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.6795 - accuracy: 0.4940 - val_loss: 4.4060 - val_accuracy: 0.1099 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.6527 - accuracy: 0.5054 - val_loss: 4.4375 - val_accuracy: 0.0898 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.6219 - accuracy: 0.5122 - val_loss: 3.5053 - val_accuracy: 0.1676 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.5997 - accuracy: 0.5228 - val_loss: 4.3137 - val_accuracy: 0.1199 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.5198 - accuracy: 0.5409 - val_loss: 3.9728 - val_accuracy: 0.1385 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.5888 - accuracy: 0.5285 - val_loss: 2.1798 - val_accuracy: 0.3683 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.5421 - accuracy: 0.5376 - val_loss: 2.8181 - val_accuracy: 0.2353 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.4935 - accuracy: 0.5511 - val_loss: 4.3897 - val_accuracy: 0.1867 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1.4569 - accuracy: 0.5661 - val_loss: 4.1782 - val_accuracy: 0.1194 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.4148 - accuracy: 0.5706 - val_loss: 4.1497 - val_accuracy: 0.1530 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.3795 - accuracy: 0.5863 - val_loss: 2.0730 - val_accuracy: 0.4155 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.3581 - accuracy: 0.5947 - val_loss: 3.1523 - val_accuracy: 0.2358 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.3892 - accuracy: 0.5823 - val_loss: 4.4175 - val_accuracy: 0.1661 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.4069 - accuracy: 0.5719 - val_loss: 2.6768 - val_accuracy: 0.3111 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.3780 - accuracy: 0.5797 - val_loss: 2.1476 - val_accuracy: 0.3859 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2942 - accuracy: 0.6074 - val_loss: 2.2993 - val_accuracy: 0.3457 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.3041 - accuracy: 0.6066 - val_loss: 2.5976 - val_accuracy: 0.3773 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.2611 - accuracy: 0.6214 - val_loss: 2.8531 - val_accuracy: 0.3281 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2445 - accuracy: 0.6214 - val_loss: 2.5329 - val_accuracy: 0.3994 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2473 - accuracy: 0.6227 - val_loss: 4.4819 - val_accuracy: 0.1977 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.2691 - accuracy: 0.6158 - val_loss: 3.1625 - val_accuracy: 0.2704 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2232 - accuracy: 0.6283 - val_loss: 4.4479 - val_accuracy: 0.1656 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2235 - accuracy: 0.6313 - val_loss: 3.3525 - val_accuracy: 0.2263 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.2259 - accuracy: 0.6277 - val_loss: 2.3473 - val_accuracy: 0.3552 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.1917 - accuracy: 0.6386 - val_loss: 4.1074 - val_accuracy: 0.1621 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2050 - accuracy: 0.6334 - val_loss: 2.6640 - val_accuracy: 0.3382 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1889 - accuracy: 0.6423 - val_loss: 2.1666 - val_accuracy: 0.3828 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.1912 - accuracy: 0.6421 - val_loss: 2.5516 - val_accuracy: 0.3387 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.1493 - accuracy: 0.6549 - val_loss: 2.8114 - val_accuracy: 0.3056 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.1174 - accuracy: 0.6605 - val_loss: 2.3602 - val_accuracy: 0.3723 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1263 - accuracy: 0.6586 - val_loss: 2.3414 - val_accuracy: 0.3904 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0839 - accuracy: 0.6768 - val_loss: 1.9748 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0652 - accuracy: 0.6753 - val_loss: 3.4216 - val_accuracy: 0.2679 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0615 - accuracy: 0.6795 - val_loss: 6.3611 - val_accuracy: 0.1445 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0327 - accuracy: 0.6914 - val_loss: 2.8259 - val_accuracy: 0.3583 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0158 - accuracy: 0.6953 - val_loss: 4.2136 - val_accuracy: 0.2303 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0167 - accuracy: 0.6940 - val_loss: 2.2607 - val_accuracy: 0.4596 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0259 - accuracy: 0.6864 - val_loss: 3.1059 - val_accuracy: 0.2840 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0504 - accuracy: 0.6798 - val_loss: 6.2830 - val_accuracy: 0.1189 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0328 - accuracy: 0.6884 - val_loss: 2.7543 - val_accuracy: 0.4104 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0141 - accuracy: 0.6901 - val_loss: 3.5821 - val_accuracy: 0.2935 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9860 - accuracy: 0.7013 - val_loss: 4.3199 - val_accuracy: 0.2283 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0119 - accuracy: 0.6930 - val_loss: 2.5538 - val_accuracy: 0.4134 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9395 - accuracy: 0.7139 - val_loss: 2.2958 - val_accuracy: 0.4541 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9641 - accuracy: 0.7135 - val_loss: 3.3550 - val_accuracy: 0.3557 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9497 - accuracy: 0.7111 - val_loss: 2.4026 - val_accuracy: 0.4134 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0374 - accuracy: 0.6870 - val_loss: 3.5296 - val_accuracy: 0.2393 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.0288 - accuracy: 0.6873 - val_loss: 3.0178 - val_accuracy: 0.3377 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9726 - accuracy: 0.7048 - val_loss: 2.3653 - val_accuracy: 0.3954 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9354 - accuracy: 0.7166 - val_loss: 2.9368 - val_accuracy: 0.3402 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9233 - accuracy: 0.7195 - val_loss: 4.0249 - val_accuracy: 0.2659 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9306 - accuracy: 0.7159 - val_loss: 5.8567 - val_accuracy: 0.1641 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9423 - accuracy: 0.7099 - val_loss: 3.9650 - val_accuracy: 0.3136 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9080 - accuracy: 0.7220 - val_loss: 3.6757 - val_accuracy: 0.4205 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9242 - accuracy: 0.7185 - val_loss: 2.6092 - val_accuracy: 0.3964 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8571 - accuracy: 0.7409 - val_loss: 3.1556 - val_accuracy: 0.3823 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.7431\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8515 - accuracy: 0.7431 - val_loss: 2.8203 - val_accuracy: 0.3813 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8870 - accuracy: 0.7291 - val_loss: 1.7801 - val_accuracy: 0.5178 - lr: 5.0000e-04\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8258 - accuracy: 0.7485 - val_loss: 2.0921 - val_accuracy: 0.5043 - lr: 5.0000e-04\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7697 - accuracy: 0.7660 - val_loss: 1.5526 - val_accuracy: 0.5951 - lr: 5.0000e-04\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7515 - accuracy: 0.7691 - val_loss: 2.0692 - val_accuracy: 0.4972 - lr: 5.0000e-04\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7604 - accuracy: 0.7707 - val_loss: 1.8038 - val_accuracy: 0.5640 - lr: 5.0000e-04\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.7413 - accuracy: 0.7705 - val_loss: 2.0722 - val_accuracy: 0.5253 - lr: 5.0000e-04\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7405 - accuracy: 0.7784 - val_loss: 1.7349 - val_accuracy: 0.5730 - lr: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7504 - accuracy: 0.7716 - val_loss: 1.7426 - val_accuracy: 0.5615 - lr: 5.0000e-04\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.7214 - accuracy: 0.7792 - val_loss: 1.5014 - val_accuracy: 0.6237 - lr: 5.0000e-04\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7316 - accuracy: 0.7758 - val_loss: 2.2792 - val_accuracy: 0.4536 - lr: 5.0000e-04\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7449 - accuracy: 0.7723 - val_loss: 2.4033 - val_accuracy: 0.4681 - lr: 5.0000e-04\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7553 - accuracy: 0.7726 - val_loss: 1.9451 - val_accuracy: 0.5304 - lr: 5.0000e-04\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7327 - accuracy: 0.7765 - val_loss: 1.8414 - val_accuracy: 0.5745 - lr: 5.0000e-04\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7146 - accuracy: 0.7814 - val_loss: 2.1518 - val_accuracy: 0.4867 - lr: 5.0000e-04\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6963 - accuracy: 0.7880 - val_loss: 1.4173 - val_accuracy: 0.6563 - lr: 5.0000e-04\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7051 - accuracy: 0.7851 - val_loss: 1.9284 - val_accuracy: 0.5354 - lr: 5.0000e-04\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6963 - accuracy: 0.7842 - val_loss: 1.8459 - val_accuracy: 0.5840 - lr: 5.0000e-04\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7077 - accuracy: 0.7801 - val_loss: 1.6991 - val_accuracy: 0.5911 - lr: 5.0000e-04\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6903 - accuracy: 0.7844 - val_loss: 1.6777 - val_accuracy: 0.5911 - lr: 5.0000e-04\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6972 - accuracy: 0.7829 - val_loss: 1.5008 - val_accuracy: 0.6197 - lr: 5.0000e-04\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.7162 - accuracy: 0.7814 - val_loss: 3.5267 - val_accuracy: 0.3879 - lr: 5.0000e-04\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.7030 - accuracy: 0.7835 - val_loss: 2.3600 - val_accuracy: 0.4812 - lr: 5.0000e-04\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6701 - accuracy: 0.7946 - val_loss: 1.4935 - val_accuracy: 0.6523 - lr: 5.0000e-04\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6797 - accuracy: 0.7929 - val_loss: 2.7850 - val_accuracy: 0.4250 - lr: 5.0000e-04\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6739 - accuracy: 0.7941 - val_loss: 2.2523 - val_accuracy: 0.4867 - lr: 5.0000e-04\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6868 - accuracy: 0.7924 - val_loss: 2.0229 - val_accuracy: 0.5710 - lr: 5.0000e-04\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7816 - accuracy: 0.7646 - val_loss: 3.8095 - val_accuracy: 0.3823 - lr: 5.0000e-04\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.7375 - accuracy: 0.7767 - val_loss: 2.5434 - val_accuracy: 0.4501 - lr: 5.0000e-04\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.6743 - accuracy: 0.7926 - val_loss: 1.7208 - val_accuracy: 0.5850 - lr: 5.0000e-04\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6556 - accuracy: 0.7991 - val_loss: 1.8456 - val_accuracy: 0.5524 - lr: 5.0000e-04\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6457 - accuracy: 0.8044 - val_loss: 1.6371 - val_accuracy: 0.6152 - lr: 5.0000e-04\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6611 - accuracy: 0.8003 - val_loss: 2.4097 - val_accuracy: 0.4676 - lr: 5.0000e-04\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6436 - accuracy: 0.8029 - val_loss: 1.6785 - val_accuracy: 0.6036 - lr: 5.0000e-04\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6180 - accuracy: 0.8114 - val_loss: 1.6146 - val_accuracy: 0.6061 - lr: 5.0000e-04\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.6488 - accuracy: 0.7995 - val_loss: 2.8770 - val_accuracy: 0.4240 - lr: 5.0000e-04\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7179 - accuracy: 0.7760 - val_loss: 2.5048 - val_accuracy: 0.4431 - lr: 5.0000e-04\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7209 - accuracy: 0.7797 - val_loss: 3.1209 - val_accuracy: 0.3909 - lr: 5.0000e-04\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6588 - accuracy: 0.7978 - val_loss: 1.7914 - val_accuracy: 0.5936 - lr: 5.0000e-04\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6358 - accuracy: 0.8049 - val_loss: 1.7354 - val_accuracy: 0.5896 - lr: 5.0000e-04\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6317 - accuracy: 0.8068\n",
      "Epoch 130: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6317 - accuracy: 0.8068 - val_loss: 1.7700 - val_accuracy: 0.6046 - lr: 5.0000e-04\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5887 - accuracy: 0.8224 - val_loss: 1.5718 - val_accuracy: 0.6006 - lr: 2.5000e-04\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5825 - accuracy: 0.8252 - val_loss: 1.4502 - val_accuracy: 0.6548 - lr: 2.5000e-04\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.5626 - accuracy: 0.8254 - val_loss: 1.3111 - val_accuracy: 0.7030 - lr: 2.5000e-04\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5627 - accuracy: 0.8293 - val_loss: 1.6107 - val_accuracy: 0.6152 - lr: 2.5000e-04\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.5610 - accuracy: 0.8294 - val_loss: 1.5372 - val_accuracy: 0.6543 - lr: 2.5000e-04\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.5613 - accuracy: 0.8270 - val_loss: 1.3902 - val_accuracy: 0.6839 - lr: 2.5000e-04\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5749 - accuracy: 0.8222 - val_loss: 1.4410 - val_accuracy: 0.6729 - lr: 2.5000e-04\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5633 - accuracy: 0.8297 - val_loss: 1.4287 - val_accuracy: 0.6959 - lr: 2.5000e-04\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5405 - accuracy: 0.8369 - val_loss: 1.4379 - val_accuracy: 0.6864 - lr: 2.5000e-04\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5360 - accuracy: 0.8380 - val_loss: 1.4818 - val_accuracy: 0.6713 - lr: 2.5000e-04\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.5463 - accuracy: 0.8308 - val_loss: 1.6933 - val_accuracy: 0.6272 - lr: 2.5000e-04\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5367 - accuracy: 0.8342 - val_loss: 1.8863 - val_accuracy: 0.5921 - lr: 2.5000e-04\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5504 - accuracy: 0.8336 - val_loss: 1.2888 - val_accuracy: 0.7195 - lr: 2.5000e-04\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5369 - accuracy: 0.8327 - val_loss: 1.8144 - val_accuracy: 0.6187 - lr: 2.5000e-04\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5308 - accuracy: 0.8368 - val_loss: 1.2976 - val_accuracy: 0.7045 - lr: 2.5000e-04\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.5347 - accuracy: 0.8309 - val_loss: 1.6727 - val_accuracy: 0.6372 - lr: 2.5000e-04\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5256 - accuracy: 0.8436 - val_loss: 1.5105 - val_accuracy: 0.6834 - lr: 2.5000e-04\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5024 - accuracy: 0.8477 - val_loss: 1.4269 - val_accuracy: 0.7015 - lr: 2.5000e-04\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5164 - accuracy: 0.8417 - val_loss: 1.5292 - val_accuracy: 0.6618 - lr: 2.5000e-04\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.5245 - accuracy: 0.8402 - val_loss: 2.0989 - val_accuracy: 0.5815 - lr: 2.5000e-04\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5173 - accuracy: 0.8395 - val_loss: 1.8333 - val_accuracy: 0.6267 - lr: 2.5000e-04\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5386 - accuracy: 0.8330 - val_loss: 2.3368 - val_accuracy: 0.5263 - lr: 2.5000e-04\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5326 - accuracy: 0.8335 - val_loss: 2.5298 - val_accuracy: 0.5309 - lr: 2.5000e-04\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5146 - accuracy: 0.8373 - val_loss: 1.2990 - val_accuracy: 0.7175 - lr: 2.5000e-04\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5184 - accuracy: 0.8393 - val_loss: 1.4652 - val_accuracy: 0.6949 - lr: 2.5000e-04\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5005 - accuracy: 0.8477 - val_loss: 1.3322 - val_accuracy: 0.7180 - lr: 2.5000e-04\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.5113 - accuracy: 0.8389 - val_loss: 1.6873 - val_accuracy: 0.6463 - lr: 2.5000e-04\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.5225 - accuracy: 0.8394 - val_loss: 2.0717 - val_accuracy: 0.5750 - lr: 2.5000e-04\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5362 - accuracy: 0.8321 - val_loss: 1.7989 - val_accuracy: 0.6247 - lr: 2.5000e-04\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5253 - accuracy: 0.8386 - val_loss: 1.4313 - val_accuracy: 0.6994 - lr: 2.5000e-04\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5117 - accuracy: 0.8389 - val_loss: 1.4663 - val_accuracy: 0.6994 - lr: 2.5000e-04\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5022 - accuracy: 0.8466 - val_loss: 1.3695 - val_accuracy: 0.7210 - lr: 2.5000e-04\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.4998 - accuracy: 0.8468 - val_loss: 1.8780 - val_accuracy: 0.6116 - lr: 2.5000e-04\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4942 - accuracy: 0.8496 - val_loss: 1.5349 - val_accuracy: 0.6663 - lr: 2.5000e-04\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.4979 - accuracy: 0.8414 - val_loss: 1.3911 - val_accuracy: 0.7035 - lr: 2.5000e-04\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5081 - accuracy: 0.8446 - val_loss: 1.8783 - val_accuracy: 0.6402 - lr: 2.5000e-04\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5085 - accuracy: 0.8416 - val_loss: 2.3452 - val_accuracy: 0.5263 - lr: 2.5000e-04\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5163 - accuracy: 0.8437\n",
      "Epoch 168: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5163 - accuracy: 0.8437 - val_loss: 2.1829 - val_accuracy: 0.5886 - lr: 2.5000e-04\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.4919 - accuracy: 0.8492 - val_loss: 1.3812 - val_accuracy: 0.6924 - lr: 1.2500e-04\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4962 - accuracy: 0.8429 - val_loss: 1.5853 - val_accuracy: 0.6984 - lr: 1.2500e-04\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4594 - accuracy: 0.8578 - val_loss: 1.4123 - val_accuracy: 0.7205 - lr: 1.2500e-04\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.4725 - accuracy: 0.8559 - val_loss: 1.3743 - val_accuracy: 0.7230 - lr: 1.2500e-04\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4623 - accuracy: 0.8586 - val_loss: 1.3640 - val_accuracy: 0.7230 - lr: 1.2500e-04\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4772 - accuracy: 0.8559 - val_loss: 1.2681 - val_accuracy: 0.7336 - lr: 1.2500e-04\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4694 - accuracy: 0.8583 - val_loss: 1.4537 - val_accuracy: 0.6959 - lr: 1.2500e-04\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4698 - accuracy: 0.8523 - val_loss: 1.2918 - val_accuracy: 0.7416 - lr: 1.2500e-04\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4581 - accuracy: 0.8585 - val_loss: 1.4493 - val_accuracy: 0.6954 - lr: 1.2500e-04\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4608 - accuracy: 0.8583 - val_loss: 1.3686 - val_accuracy: 0.7316 - lr: 1.2500e-04\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4652 - accuracy: 0.8552 - val_loss: 1.2498 - val_accuracy: 0.7456 - lr: 1.2500e-04\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.4523 - accuracy: 0.8631 - val_loss: 1.4519 - val_accuracy: 0.7075 - lr: 1.2500e-04\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4623 - accuracy: 0.8601 - val_loss: 1.4561 - val_accuracy: 0.7030 - lr: 1.2500e-04\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4526 - accuracy: 0.8621 - val_loss: 1.3670 - val_accuracy: 0.7215 - lr: 1.2500e-04\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4512 - accuracy: 0.8624 - val_loss: 1.3968 - val_accuracy: 0.7100 - lr: 1.2500e-04\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4490 - accuracy: 0.8620 - val_loss: 1.3724 - val_accuracy: 0.7175 - lr: 1.2500e-04\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4572 - accuracy: 0.8574 - val_loss: 1.5897 - val_accuracy: 0.6739 - lr: 1.2500e-04\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4502 - accuracy: 0.8624 - val_loss: 1.4769 - val_accuracy: 0.7010 - lr: 1.2500e-04\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4568 - accuracy: 0.8597 - val_loss: 1.3072 - val_accuracy: 0.7466 - lr: 1.2500e-04\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4407 - accuracy: 0.8630 - val_loss: 1.5170 - val_accuracy: 0.6889 - lr: 1.2500e-04\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.4403 - accuracy: 0.8657 - val_loss: 1.2950 - val_accuracy: 0.7381 - lr: 1.2500e-04\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4422 - accuracy: 0.8633 - val_loss: 1.3444 - val_accuracy: 0.7301 - lr: 1.2500e-04\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4449 - accuracy: 0.8595 - val_loss: 1.3588 - val_accuracy: 0.7346 - lr: 1.2500e-04\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4557 - accuracy: 0.8601 - val_loss: 1.4414 - val_accuracy: 0.7280 - lr: 1.2500e-04\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.4368 - accuracy: 0.8628 - val_loss: 1.4800 - val_accuracy: 0.7175 - lr: 1.2500e-04\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.4517 - accuracy: 0.8629 - val_loss: 1.4272 - val_accuracy: 0.7346 - lr: 1.2500e-04\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4349 - accuracy: 0.8631 - val_loss: 1.4113 - val_accuracy: 0.7291 - lr: 1.2500e-04\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4359 - accuracy: 0.8613 - val_loss: 1.7839 - val_accuracy: 0.6739 - lr: 1.2500e-04\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.4448 - accuracy: 0.8614 - val_loss: 1.6032 - val_accuracy: 0.7025 - lr: 1.2500e-04\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4253 - accuracy: 0.8672 - val_loss: 1.3494 - val_accuracy: 0.7321 - lr: 1.2500e-04\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4252 - accuracy: 0.8732 - val_loss: 1.4432 - val_accuracy: 0.7190 - lr: 1.2500e-04\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4189 - accuracy: 0.8743 - val_loss: 1.4368 - val_accuracy: 0.7155 - lr: 1.2500e-04\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4320 - accuracy: 0.8656 - val_loss: 1.3738 - val_accuracy: 0.7321 - lr: 1.2500e-04\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4320 - accuracy: 0.8682 - val_loss: 1.4616 - val_accuracy: 0.7110 - lr: 1.2500e-04\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4208 - accuracy: 0.8756 - val_loss: 1.4906 - val_accuracy: 0.7155 - lr: 1.2500e-04\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4306 - accuracy: 0.8676\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4306 - accuracy: 0.8676 - val_loss: 1.3672 - val_accuracy: 0.7301 - lr: 1.2500e-04\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.4199 - accuracy: 0.8684 - val_loss: 1.4129 - val_accuracy: 0.7275 - lr: 6.2500e-05\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.4129 - accuracy: 0.8732 - val_loss: 1.3314 - val_accuracy: 0.7386 - lr: 6.2500e-05\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4096 - accuracy: 0.8735 - val_loss: 1.3239 - val_accuracy: 0.7511 - lr: 6.2500e-05\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.4086 - accuracy: 0.8742 - val_loss: 1.3606 - val_accuracy: 0.7376 - lr: 6.2500e-05\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4130 - accuracy: 0.8719 - val_loss: 1.4158 - val_accuracy: 0.7306 - lr: 6.2500e-05\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4145 - accuracy: 0.8714 - val_loss: 1.2933 - val_accuracy: 0.7491 - lr: 6.2500e-05\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4068 - accuracy: 0.8745 - val_loss: 1.3167 - val_accuracy: 0.7466 - lr: 6.2500e-05\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.4046 - accuracy: 0.8762 - val_loss: 1.3867 - val_accuracy: 0.7341 - lr: 6.2500e-05\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4245 - accuracy: 0.8680 - val_loss: 1.3514 - val_accuracy: 0.7551 - lr: 6.2500e-05\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.4049 - accuracy: 0.8731 - val_loss: 1.3811 - val_accuracy: 0.7416 - lr: 6.2500e-05\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4071 - accuracy: 0.8726 - val_loss: 1.3973 - val_accuracy: 0.7296 - lr: 6.2500e-05\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4183 - accuracy: 0.8735 - val_loss: 1.4108 - val_accuracy: 0.7401 - lr: 6.2500e-05\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4184 - accuracy: 0.8698 - val_loss: 1.4965 - val_accuracy: 0.7265 - lr: 6.2500e-05\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4084 - accuracy: 0.8761 - val_loss: 1.3957 - val_accuracy: 0.7301 - lr: 6.2500e-05\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3964 - accuracy: 0.8780 - val_loss: 1.3475 - val_accuracy: 0.7411 - lr: 6.2500e-05\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4115 - accuracy: 0.8698 - val_loss: 1.3514 - val_accuracy: 0.7386 - lr: 6.2500e-05\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.4121 - accuracy: 0.8743 - val_loss: 1.3512 - val_accuracy: 0.7401 - lr: 6.2500e-05\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.3977 - accuracy: 0.8795 - val_loss: 1.3618 - val_accuracy: 0.7321 - lr: 6.2500e-05\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4019 - accuracy: 0.8775 - val_loss: 1.3527 - val_accuracy: 0.7456 - lr: 6.2500e-05\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4009 - accuracy: 0.8786 - val_loss: 1.3443 - val_accuracy: 0.7431 - lr: 6.2500e-05\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3968 - accuracy: 0.8786 - val_loss: 1.3996 - val_accuracy: 0.7275 - lr: 6.2500e-05\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4110 - accuracy: 0.8704 - val_loss: 1.3261 - val_accuracy: 0.7451 - lr: 6.2500e-05\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4052 - accuracy: 0.8755 - val_loss: 1.3752 - val_accuracy: 0.7391 - lr: 6.2500e-05\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.4165 - accuracy: 0.8722 - val_loss: 1.3983 - val_accuracy: 0.7331 - lr: 6.2500e-05\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8712\n",
      "Epoch 229: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4133 - accuracy: 0.8712 - val_loss: 1.3839 - val_accuracy: 0.7291 - lr: 6.2500e-05\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3955 - accuracy: 0.8804 - val_loss: 1.3327 - val_accuracy: 0.7396 - lr: 3.1250e-05\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.3877 - accuracy: 0.8782 - val_loss: 1.2840 - val_accuracy: 0.7471 - lr: 3.1250e-05\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3966 - accuracy: 0.8780 - val_loss: 1.3074 - val_accuracy: 0.7466 - lr: 3.1250e-05\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.3987 - accuracy: 0.8779 - val_loss: 1.3352 - val_accuracy: 0.7456 - lr: 3.1250e-05\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3958 - accuracy: 0.8736 - val_loss: 1.3660 - val_accuracy: 0.7426 - lr: 3.1250e-05\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.4011 - accuracy: 0.8741 - val_loss: 1.3368 - val_accuracy: 0.7471 - lr: 3.1250e-05\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3847 - accuracy: 0.8823 - val_loss: 1.3433 - val_accuracy: 0.7441 - lr: 3.1250e-05\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.3936 - accuracy: 0.8796 - val_loss: 1.4280 - val_accuracy: 0.7376 - lr: 3.1250e-05\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.3936 - accuracy: 0.8783 - val_loss: 1.3677 - val_accuracy: 0.7436 - lr: 3.1250e-05\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3961 - accuracy: 0.8781 - val_loss: 1.3375 - val_accuracy: 0.7466 - lr: 3.1250e-05\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3980 - accuracy: 0.8781 - val_loss: 1.3056 - val_accuracy: 0.7461 - lr: 3.1250e-05\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3969 - accuracy: 0.8757 - val_loss: 1.3906 - val_accuracy: 0.7331 - lr: 3.1250e-05\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.3882 - accuracy: 0.8817 - val_loss: 1.3279 - val_accuracy: 0.7421 - lr: 3.1250e-05\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4055 - accuracy: 0.8771 - val_loss: 1.3637 - val_accuracy: 0.7351 - lr: 3.1250e-05\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.3882 - accuracy: 0.8812 - val_loss: 1.3565 - val_accuracy: 0.7416 - lr: 3.1250e-05\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3926 - accuracy: 0.8770 - val_loss: 1.3953 - val_accuracy: 0.7516 - lr: 3.1250e-05\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.3907 - accuracy: 0.8799 - val_loss: 1.4296 - val_accuracy: 0.7451 - lr: 3.1250e-05\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3999 - accuracy: 0.8755 - val_loss: 1.3914 - val_accuracy: 0.7456 - lr: 3.1250e-05\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4023 - accuracy: 0.8785 - val_loss: 1.4735 - val_accuracy: 0.7255 - lr: 3.1250e-05\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.3850 - accuracy: 0.8798 - val_loss: 1.4053 - val_accuracy: 0.7386 - lr: 3.1250e-05\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.3932 - accuracy: 0.8789 - val_loss: 1.3613 - val_accuracy: 0.7406 - lr: 3.1250e-05\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3908 - accuracy: 0.8797 - val_loss: 1.3664 - val_accuracy: 0.7481 - lr: 3.1250e-05\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.3759 - accuracy: 0.8860 - val_loss: 1.4851 - val_accuracy: 0.7316 - lr: 3.1250e-05\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3945 - accuracy: 0.8770 - val_loss: 1.3693 - val_accuracy: 0.7436 - lr: 3.1250e-05\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8823\n",
      "Epoch 254: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3848 - accuracy: 0.8823 - val_loss: 1.4202 - val_accuracy: 0.7265 - lr: 3.1250e-05\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3890 - accuracy: 0.8774 - val_loss: 1.3991 - val_accuracy: 0.7326 - lr: 1.5625e-05\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.3874 - accuracy: 0.8799 - val_loss: 1.3768 - val_accuracy: 0.7441 - lr: 1.5625e-05\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3909 - accuracy: 0.8783 - val_loss: 1.3264 - val_accuracy: 0.7501 - lr: 1.5625e-05\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.3904 - accuracy: 0.8818 - val_loss: 1.3495 - val_accuracy: 0.7436 - lr: 1.5625e-05\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3852 - accuracy: 0.8818 - val_loss: 1.3734 - val_accuracy: 0.7446 - lr: 1.5625e-05\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3858 - accuracy: 0.8799 - val_loss: 1.3787 - val_accuracy: 0.7451 - lr: 1.5625e-05\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3836 - accuracy: 0.8825 - val_loss: 1.3607 - val_accuracy: 0.7511 - lr: 1.5625e-05\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3907 - accuracy: 0.8813 - val_loss: 1.3616 - val_accuracy: 0.7496 - lr: 1.5625e-05\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.3826 - accuracy: 0.8804 - val_loss: 1.3331 - val_accuracy: 0.7486 - lr: 1.5625e-05\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3802 - accuracy: 0.8849 - val_loss: 1.3200 - val_accuracy: 0.7481 - lr: 1.5625e-05\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.3870 - accuracy: 0.8780 - val_loss: 1.3150 - val_accuracy: 0.7496 - lr: 1.5625e-05\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.3856 - accuracy: 0.8826 - val_loss: 1.3206 - val_accuracy: 0.7461 - lr: 1.5625e-05\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.3756 - accuracy: 0.8853 - val_loss: 1.3267 - val_accuracy: 0.7471 - lr: 1.5625e-05\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.3704 - accuracy: 0.8843 - val_loss: 1.3497 - val_accuracy: 0.7451 - lr: 1.5625e-05\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.3826 - accuracy: 0.8818 - val_loss: 1.3420 - val_accuracy: 0.7446 - lr: 1.5625e-05\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3809 - accuracy: 0.8833 - val_loss: 1.3188 - val_accuracy: 0.7481 - lr: 1.5625e-05\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3769 - accuracy: 0.8830 - val_loss: 1.3173 - val_accuracy: 0.7491 - lr: 1.5625e-05\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3838 - accuracy: 0.8824 - val_loss: 1.3520 - val_accuracy: 0.7446 - lr: 1.5625e-05\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.3746 - accuracy: 0.8824 - val_loss: 1.3159 - val_accuracy: 0.7501 - lr: 1.5625e-05\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.3733 - accuracy: 0.8864 - val_loss: 1.3117 - val_accuracy: 0.7551 - lr: 1.5625e-05\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.3757 - accuracy: 0.8853 - val_loss: 1.3879 - val_accuracy: 0.7451 - lr: 1.5625e-05\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3810 - accuracy: 0.8802 - val_loss: 1.3871 - val_accuracy: 0.7436 - lr: 1.5625e-05\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3806 - accuracy: 0.8835 - val_loss: 1.3131 - val_accuracy: 0.7556 - lr: 1.5625e-05\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.3776 - accuracy: 0.8838 - val_loss: 1.3273 - val_accuracy: 0.7521 - lr: 1.5625e-05\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8814\n",
      "Epoch 279: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.3857 - accuracy: 0.8814 - val_loss: 1.3669 - val_accuracy: 0.7401 - lr: 1.5625e-05\n",
      "Epoch 279: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Enable mixed precision\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "epochs = 1000  # Adjust number of epochs as needed\n",
    "batch_size = 1024\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 128)           916992    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 128)           0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 10, 128)           512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               394240    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 53)                6837      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1351477 (5.16 MB)\n",
      "Trainable params: 1351221 (5.15 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the model data for later use in the Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'model-' + MODEL_VERSION + '/'\n",
    "model_name = 'model-' + MODEL_VERSION + '.h5'\n",
    "model.save(model_dir + model_name)\n",
    "labels_name = 'labels-' + MODEL_VERSION + '.npy'\n",
    "np.save(model_dir + labels_name, actions)\n",
    "\n",
    "df_csv = pd.DataFrame(actions)\n",
    "labels_csv_name = 'labels-' + MODEL_VERSION + '.csv'\n",
    "df_csv.to_csv(model_dir + labels_csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        aunt       0.92      0.83      0.87        41\n",
      "        bird       0.85      0.72      0.78        46\n",
      "       black       0.74      0.70      0.72        20\n",
      "     brother       0.61      0.67      0.64        21\n",
      "       brown       0.86      0.95      0.90        44\n",
      "         bug       0.87      0.82      0.85        40\n",
      " callonphone       0.81      0.71      0.76        42\n",
      "       cheek       0.69      0.67      0.68        36\n",
      "       clown       0.84      0.84      0.84        61\n",
      "         cow       0.76      0.74      0.75        46\n",
      "        cute       0.90      0.76      0.83        34\n",
      "         dad       0.60      0.68      0.64        38\n",
      "        doll       0.78      0.58      0.67        43\n",
      "      donkey       0.61      0.79      0.69        34\n",
      "       drink       0.76      0.70      0.73        37\n",
      "         ear       0.83      0.89      0.86        44\n",
      "         eye       0.81      0.69      0.75        32\n",
      "        feet       0.82      0.85      0.84        39\n",
      "        find       0.86      0.76      0.81        33\n",
      "     fireman       0.64      0.81      0.71        37\n",
      "      flower       0.79      0.74      0.76        35\n",
      "         for       0.81      0.55      0.65        31\n",
      "        frog       0.83      0.78      0.81        37\n",
      "     grandpa       0.63      0.65      0.64        26\n",
      "       grass       0.74      0.72      0.73        32\n",
      "         gum       0.69      0.80      0.74        44\n",
      "        hair       0.85      0.79      0.81        42\n",
      "         hen       0.82      0.66      0.73        41\n",
      "        home       0.57      0.71      0.63        24\n",
      "       horse       0.81      0.68      0.74        50\n",
      "        lamp       0.53      0.55      0.54        31\n",
      "         mad       0.53      0.55      0.54        31\n",
      "         mom       0.66      0.66      0.66        35\n",
      "       mouse       0.66      0.71      0.68        58\n",
      "        nose       0.95      0.83      0.89        42\n",
      "         owl       0.86      0.80      0.83        46\n",
      "         pig       0.82      0.86      0.84        36\n",
      "      police       0.79      0.78      0.79        54\n",
      "       radio       0.78      0.86      0.82        44\n",
      "         see       0.76      0.93      0.84        41\n",
      "        shhh       0.78      0.90      0.83        48\n",
      "       shirt       0.63      0.94      0.76        18\n",
      "        sick       0.83      0.83      0.83        29\n",
      "      stairs       0.72      0.66      0.69        35\n",
      "       stuck       0.68      0.71      0.70        21\n",
      "       taste       0.78      0.86      0.82        37\n",
      "     thirsty       0.69      0.72      0.71        25\n",
      "       tiger       0.74      0.72      0.73        36\n",
      "       uncle       0.77      0.79      0.78        52\n",
      "       water       0.88      0.88      0.88        48\n",
      "         who       0.70      0.77      0.73        48\n",
      "       yucky       0.80      0.64      0.71        25\n",
      "       zebra       0.72      0.75      0.73        24\n",
      "\n",
      "    accuracy                           0.76      1994\n",
      "   macro avg       0.76      0.75      0.75      1994\n",
      "weighted avg       0.77      0.76      0.76      1994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "precision = precision_score(true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "f1 = f1_score(true_classes, predicted_classes, average='weighted')\n",
    "report = classification_report(true_classes, predicted_classes, target_names=actions)\n",
    "\n",
    "with open(model_dir + 'model_metrics.txt', 'w') as file:\n",
    "    file.write(f\"Accuracy: {accuracy}\\n\")\n",
    "    file.write(f\"Precision: {precision}\\n\")\n",
    "    file.write(f\"Recall: {recall}\\n\")\n",
    "    file.write(f\"F1 Score: {f1}\\n\")\n",
    "    file.write(f\"\\nClassification Report:\\n{report}\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    15\n",
      "recall       15\n",
      "f1-score     15\n",
      "support      15\n",
      "dtype: int64\n",
      "       precision    recall  f1-score  support\n",
      "taste   0.780488  0.864865  0.820513     37.0\n",
      "cute    0.896552  0.764706  0.825397     34.0\n",
      "sick    0.827586  0.827586  0.827586     29.0\n",
      "owl     0.860465  0.804348  0.831461     46.0\n",
      "shhh    0.781818  0.895833  0.834951     48.0\n",
      "see     0.760000  0.926829  0.835165     41.0\n",
      "feet    0.825000  0.846154  0.835443     39.0\n",
      "clown   0.836066  0.836066  0.836066     61.0\n",
      "pig     0.815789  0.861111  0.837838     36.0\n",
      "bug     0.868421  0.825000  0.846154     40.0\n",
      "ear     0.829787  0.886364  0.857143     44.0\n",
      "aunt    0.918919  0.829268  0.871795     41.0\n",
      "water   0.875000  0.875000  0.875000     48.0\n",
      "nose    0.945946  0.833333  0.886076     42.0\n",
      "brown   0.857143  0.954545  0.903226     44.0\n",
      "['taste' 'cute' 'sick' 'owl' 'shhh' 'see' 'feet' 'clown' 'pig' 'bug' 'ear'\n",
      " 'aunt' 'water' 'nose' 'brown']\n"
     ]
    }
   ],
   "source": [
    "if (save_high_performers):\n",
    "    # Generate the classification report\n",
    "    report = classification_report(true_classes, predicted_classes, target_names=actions, output_dict=True)\n",
    "\n",
    "    # Convert report to a DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # The last few rows are overall metrics (accuracy, macro avg, weighted avg), so we remove them\n",
    "    report_df = report_df[:-3]\n",
    "\n",
    "    # Display the DataFrame sorted by F1-score\n",
    "    report_df_sorted = report_df.sort_values(by='f1-score')\n",
    "\n",
    "    high_performance_threshold = 0.82  # Define your threshold\n",
    "    high_performing_categories = report_df_sorted[report_df_sorted['f1-score'] > high_performance_threshold]\n",
    "    print(high_performing_categories.count())\n",
    "    print(high_performing_categories)\n",
    "\n",
    "    high_performing_labels = high_performing_categories.index.to_numpy()\n",
    "    print(high_performing_labels)\n",
    "\n",
    "    np.save(model_dir + 'high_performing_labels.npy', high_performing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (save_low_performers):\n",
    "    # Generate the classification report\n",
    "    report = classification_report(true_classes, predicted_classes, target_names=actions, output_dict=True)\n",
    "\n",
    "    # Convert report to a DataFrame\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # The last few rows are overall metrics (accuracy, macro avg, weighted avg), so we remove them\n",
    "    report_df = report_df[:-3]\n",
    "\n",
    "    # Display the DataFrame sorted by F1-score\n",
    "    report_df_sorted = report_df.sort_values(by='f1-score')\n",
    "\n",
    "    low_performance_threshold = 0.6  # Define your threshold\n",
    "    low_performing_categories = report_df_sorted[report_df_sorted['f1-score'] < low_performance_threshold]\n",
    "    print(low_performing_categories.count())\n",
    "    print(low_performing_categories)\n",
    "\n",
    "    low_performing_labels = low_performing_categories.index.to_numpy()\n",
    "    print(low_performing_labels)\n",
    "\n",
    "    np.save(model_dir + 'low_performing_labels.npy', low_performing_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 10\n",
    "print(actions[np.argmax(res[test_index])])\n",
    "print(actions[np.argmax(y_test[test_index])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
