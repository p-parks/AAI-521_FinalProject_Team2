{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhCHlJhISty8",
        "outputId": "9656ff78-c7b1-4651-a39d-43ccf5226647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/611.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E5VGeC4Yt6J"
      },
      "outputs": [],
      "source": [
        "!pip install -q opencv-python mediapipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWokLOlmGCuz"
      },
      "source": [
        "# Setup Model Weight Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpsAq2AAGCu0"
      },
      "outputs": [],
      "source": [
        "use_google_drive = False\n",
        "\n",
        "transformer_dir = \"./transformer/\"\n",
        "lstm_dir = \"./lstm/\"\n",
        "\n",
        "if (use_google_drive):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    transformer_dir = '/content/drive/MyDrive/Colab Notebooks/Data/asl-signs/'\n",
        "    lstm_dir = '/content/drive/MyDrive/Colab Notebooks/Data/asl-signs/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "andJbUTBShE0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapipe as mp\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "from google.colab.patches import cv2_imshow\n",
        "import PIL\n",
        "import io\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from keras.models import load_model\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3f10SU6bT4z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lvce96WxcsTG"
      },
      "outputs": [],
      "source": [
        "# Epsilon value for layer normalisation\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "\n",
        "# Dense layer units for landmarks\n",
        "LIPS_UNITS = 384\n",
        "HANDS_UNITS = 384\n",
        "POSE_UNITS = 384\n",
        "# final embedding and transformer embedding size\n",
        "UNITS = 512\n",
        "\n",
        "# Transformer\n",
        "NUM_BLOCKS = 2\n",
        "MLP_RATIO = 2\n",
        "\n",
        "# Dropout\n",
        "EMBEDDING_DROPOUT = 0.00\n",
        "MLP_DROPOUT_RATIO = 0.30\n",
        "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
        "\n",
        "# Initiailizers\n",
        "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
        "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
        "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
        "# Activations\n",
        "GELU = tf.keras.activations.gelu\n",
        "\n",
        "# If True, processing data from scratch\n",
        "# If False, loads preprocessed data\n",
        "PREPROCESS_DATA = False\n",
        "TRAIN_MODEL = True\n",
        "# True: use 10% of participants as validation set\n",
        "# False: use all data for training -> gives better LB result\n",
        "USE_VAL = False\n",
        "\n",
        "N_ROWS = 543\n",
        "N_DIMS = 3\n",
        "DIM_NAMES = ['x', 'y', 'z']\n",
        "SEED = 42\n",
        "NUM_CLASSES = 250\n",
        "IS_INTERACTIVE = True\n",
        "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
        "\n",
        "INPUT_SIZE = 64\n",
        "\n",
        "BATCH_ALL_SIGNS_N = 4\n",
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 100\n",
        "LR_MAX = 1e-3\n",
        "N_WARMUP_EPOCHS = 0\n",
        "WD_RATIO = 0.05\n",
        "MASK_VAL = 4237\n",
        "\n",
        "USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
        "START_IDX = 468\n",
        "LIPS_IDXS0 = np.array([\n",
        "        61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
        "        291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
        "        78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
        "        95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
        "    ])\n",
        "# Landmark indices in original data\n",
        "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
        "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
        "LEFT_POSE_IDXS0 = np.array([502, 504, 506, 508, 510])\n",
        "RIGHT_POSE_IDXS0 = np.array([503, 505, 507, 509, 511])\n",
        "LANDMARK_IDXS_LEFT_DOMINANT0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, LEFT_POSE_IDXS0))\n",
        "LANDMARK_IDXS_RIGHT_DOMINANT0 = np.concatenate((LIPS_IDXS0, RIGHT_HAND_IDXS0, RIGHT_POSE_IDXS0))\n",
        "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
        "N_COLS = LANDMARK_IDXS_LEFT_DOMINANT0.size\n",
        "# Landmark indices in processed data\n",
        "LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LIPS_IDXS0)).squeeze()\n",
        "LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_HAND_IDXS0)).squeeze()\n",
        "RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, RIGHT_HAND_IDXS0)).squeeze()\n",
        "HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, HAND_IDXS0)).squeeze()\n",
        "POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS_LEFT_DOMINANT0, LEFT_POSE_IDXS0)).squeeze()\n",
        "\n",
        "print(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')\n",
        "\n",
        "LIPS_START = 0\n",
        "LEFT_HAND_START = LIPS_IDXS.size\n",
        "RIGHT_HAND_START = LEFT_HAND_START + LEFT_HAND_IDXS.size\n",
        "POSE_START = RIGHT_HAND_START + RIGHT_HAND_IDXS.size\n",
        "\n",
        "print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')\n",
        "\n",
        "LIPS_MEAN = np.load(f'{transformer_dir}/LIPS_MEAN.npy')\n",
        "LIPS_STD = np.load(f'{transformer_dir}/LIPS_STD.npy')\n",
        "LEFT_HANDS_MEAN = np.load(f'{transformer_dir}/LEFT_HANDS_MEAN.npy')\n",
        "LEFT_HANDS_STD = np.load(f'{transformer_dir}/LEFT_HANDS_STD.npy')\n",
        "POSE_MEAN = np.load(f'{transformer_dir}/POSE_MEAN.npy')\n",
        "POSE_STD = np.load(f'{transformer_dir}/POSE_STD.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOOx6N91ZFIt"
      },
      "outputs": [],
      "source": [
        "class Embedding(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "\n",
        "    def get_diffs(self, l):\n",
        "        S = l.shape[2]\n",
        "        other = tf.expand_dims(l, 3)\n",
        "        other = tf.repeat(other, S, axis=3)\n",
        "        other = tf.transpose(other, [0,1,3,2])\n",
        "        diffs = tf.expand_dims(l, 3) - other\n",
        "        diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
        "        return diffs\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Positional Embedding, initialized with zeros\n",
        "        self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
        "        # Embedding layer for Landmarks\n",
        "        self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
        "        self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
        "        self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
        "        # Landmark Weights\n",
        "        self.landmark_weights = tf.Variable(tf.zeros([3], dtype=tf.float32), name='landmark_weights')\n",
        "        # Fully Connected Layers for combined landmarks\n",
        "        self.fc = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(UNITS, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
        "            tf.keras.layers.Activation(GELU),\n",
        "            tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
        "        ], name='fc')\n",
        "\n",
        "\n",
        "    def call(self, lips0, left_hand0, pose0, non_empty_frame_idxs, training=False):\n",
        "        # Lips\n",
        "        lips_embedding = self.lips_embedding(lips0)\n",
        "        # Left Hand\n",
        "        left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
        "        # Pose\n",
        "        pose_embedding = self.pose_embedding(pose0)\n",
        "        # Merge Embeddings of all landmarks with mean pooling\n",
        "        x = tf.stack((\n",
        "            lips_embedding, left_hand_embedding, pose_embedding,\n",
        "        ), axis=3)\n",
        "        x = x * tf.nn.softmax(self.landmark_weights)\n",
        "        x = tf.reduce_sum(x, axis=3)\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc(x)\n",
        "        # Add Positional Embedding\n",
        "        max_frame_idxs = tf.clip_by_value(\n",
        "                tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True),\n",
        "                1,\n",
        "                np.PINF,\n",
        "            )\n",
        "        normalised_non_empty_frame_idxs = tf.where(\n",
        "            tf.math.equal(non_empty_frame_idxs, -1.0),\n",
        "            INPUT_SIZE,\n",
        "            tf.cast(\n",
        "                non_empty_frame_idxs / max_frame_idxs * INPUT_SIZE,\n",
        "                tf.int32,\n",
        "            ),\n",
        "        )\n",
        "        x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
        "\n",
        "        return x\n",
        "\n",
        "class LandmarkEmbedding(tf.keras.Model):\n",
        "    def __init__(self, units, name):\n",
        "        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Embedding for missing landmark in frame, initizlied with zeros\n",
        "        self.empty_embedding = self.add_weight(\n",
        "            name=f'{self.name}_empty_embedding',\n",
        "            shape=[self.units],\n",
        "            initializer=INIT_ZEROS,\n",
        "        )\n",
        "        # Embedding\n",
        "        self.dense = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
        "            tf.keras.layers.Activation(GELU),\n",
        "            tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
        "        ], name=f'{self.name}_dense')\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.where(\n",
        "                # Checks whether landmark is missing in frame\n",
        "                tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
        "                # If so, the empty embedding is used\n",
        "                self.empty_embedding,\n",
        "                # Otherwise the landmark data is embedded\n",
        "                self.dense(x),\n",
        "            )\n",
        "\n",
        "# Full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_blocks):\n",
        "        super(Transformer, self).__init__(name='transformer')\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ln_1s = []\n",
        "        self.mhas = []\n",
        "        self.ln_2s = []\n",
        "        self.mlps = []\n",
        "        # Make Transformer Blocks\n",
        "        for i in range(self.num_blocks):\n",
        "            # Multi Head Attention\n",
        "            self.mhas.append(MultiHeadAttention(UNITS, 8))\n",
        "            # Multi Layer Perception\n",
        "            self.mlps.append(tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(UNITS * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
        "                tf.keras.layers.Dropout(MLP_DROPOUT_RATIO),\n",
        "                tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
        "            ]))\n",
        "\n",
        "    def call(self, x, attention_mask):\n",
        "        # Iterate input over transformer blocks\n",
        "        for mha, mlp in zip(self.mhas, self.mlps):\n",
        "            x = x + mha(x, attention_mask)\n",
        "            x = x + mlp(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n",
        "# replaced softmax with softmax layer to support masked softmax\n",
        "def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
        "    #calculates Q . K(transpose)\n",
        "    qkt = tf.matmul(q,k,transpose_b=True)\n",
        "    #caculates scaling factor\n",
        "    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
        "    scaled_qkt = qkt/dk\n",
        "    softmax = softmax(scaled_qkt, mask=attention_mask)\n",
        "\n",
        "    z = tf.matmul(softmax,v)\n",
        "    #shape: (m,Tx,depth), same shape as q,k,v\n",
        "    return z\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self,d_model,num_of_heads):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_of_heads = num_of_heads\n",
        "        self.depth = d_model//num_of_heads\n",
        "        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
        "        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
        "        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
        "        self.wo = tf.keras.layers.Dense(d_model)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self,x, attention_mask):\n",
        "\n",
        "        multi_attn = []\n",
        "        for i in range(self.num_of_heads):\n",
        "            Q = self.wq[i](x)\n",
        "            K = self.wk[i](x)\n",
        "            V = self.wv[i](x)\n",
        "            multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
        "\n",
        "        multi_head = tf.concat(multi_attn,axis=-1)\n",
        "        multi_head_attention = self.wo(multi_head)\n",
        "        return multi_head_attention\n",
        "\n",
        "# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\n",
        "def scce_with_ls(y_true, y_pred):\n",
        "    # One Hot Encode Sparsely Encoded Target Sign\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    y_true = tf.one_hot(y_true, NUM_CLASSES, axis=1)\n",
        "    y_true = tf.squeeze(y_true, axis=2)\n",
        "    # Categorical Crossentropy with native label smoothing support\n",
        "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25)\n",
        "\n",
        "def get_transformer_model():\n",
        "    # Inputs\n",
        "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
        "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
        "    # Padding Mask\n",
        "    mask0 = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
        "    mask0 = tf.expand_dims(mask0, axis=2)\n",
        "    # Random Frame Masking\n",
        "    mask = tf.where(\n",
        "        (tf.random.uniform(tf.shape(mask0)) > 0.25) & tf.math.not_equal(mask0, 0.0),\n",
        "        1.0,\n",
        "        0.0,\n",
        "    )\n",
        "    # Correct Samples Which are all masked now...\n",
        "    mask = tf.where(\n",
        "        tf.math.equal(tf.reduce_sum(mask, axis=[1,2], keepdims=True), 0.0),\n",
        "        mask0,\n",
        "        mask,\n",
        "    )\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "        left_hand: 468:489\n",
        "        pose: 489:522\n",
        "        right_hand: 522:543\n",
        "    \"\"\"\n",
        "    x = frames\n",
        "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
        "    # LIPS\n",
        "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 40, 2])\n",
        "    lips = tf.where(\n",
        "            tf.math.equal(lips, 0.0),\n",
        "            0.0,\n",
        "            (lips - LIPS_MEAN) / LIPS_STD,\n",
        "        )\n",
        "    # LEFT HAND\n",
        "    left_hand = tf.slice(x, [0,0,40,0], [-1,INPUT_SIZE, 21, 2])\n",
        "    left_hand = tf.where(\n",
        "            tf.math.equal(left_hand, 0.0),\n",
        "            0.0,\n",
        "            (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
        "        )\n",
        "    # POSE\n",
        "    pose = tf.slice(x, [0,0,61,0], [-1,INPUT_SIZE, 5, 2])\n",
        "    pose = tf.where(\n",
        "            tf.math.equal(pose, 0.0),\n",
        "            0.0,\n",
        "            (pose - POSE_MEAN) / POSE_STD,\n",
        "        )\n",
        "\n",
        "    # Flatten\n",
        "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 40*2])\n",
        "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
        "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 5*2])\n",
        "\n",
        "    # Embedding\n",
        "    x = Embedding()(lips, left_hand, pose, non_empty_frame_idxs)\n",
        "\n",
        "    # Encoder Transformer Blocks\n",
        "    x = Transformer(NUM_BLOCKS)(x, mask)\n",
        "\n",
        "    # Pooling\n",
        "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
        "    # Classifier Dropout\n",
        "    x = tf.keras.layers.Dropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
        "    # Classification Layer\n",
        "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
        "\n",
        "    outputs = x\n",
        "\n",
        "    # Create Tensorflow Model\n",
        "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
        "\n",
        "    # Sparse Categorical Cross Entropy With Label Smoothing\n",
        "    loss = scce_with_ls\n",
        "\n",
        "    # Adam Optimizer with weight decay\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
        "\n",
        "    # TopK Metrics\n",
        "    metrics = [\n",
        "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
        "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
        "    ]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjvdhPI2Z6s9"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model_transformer = get_transformer_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLkCxF_tXczQ"
      },
      "outputs": [],
      "source": [
        "model_transformer.load_weights(f'{transformer_dir}/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inSCwUlWTmF8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "signmap_sub_dir = 'sign_to_prediction_index_map.json'\n",
        "signmap_full_file_path = os.path.join(transformer_dir, signmap_sub_dir)\n",
        "# Load the sign to index mapping\n",
        "with open(signmap_full_file_path, 'r') as file:\n",
        "    sign_to_index = json.load(file)\n",
        "\n",
        "inverted_mapping = {v: k for k, v in sign_to_index.items()}\n",
        "# Convert mapping to list\n",
        "class_names = [inverted_mapping[i] for i in sorted(inverted_mapping)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TBZWf3qGCu1"
      },
      "source": [
        "## Load LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PGn9j46GCu1"
      },
      "outputs": [],
      "source": [
        "h5_file = None\n",
        "npy_file = None\n",
        "\n",
        "for file in os.listdir(lstm_dir):\n",
        "    if file.endswith('.h5') and not h5_file:\n",
        "        h5_file = file\n",
        "    elif file.endswith('.npy') and not npy_file:\n",
        "        npy_file = file\n",
        "    if h5_file and npy_file:\n",
        "        break\n",
        "\n",
        "if h5_file and npy_file:\n",
        "    model_lstm = load_model(os.path.join(lstm_dir, h5_file))\n",
        "    actions = np.load(os.path.join(lstm_dir, npy_file), allow_pickle=True)\n",
        "    print(\"Model and actions loaded successfully.\")\n",
        "else:\n",
        "    print(\"Required files not found in the folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqtZqk71baIP"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClDJvIt7ZKlo"
      },
      "outputs": [],
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
        "\n",
        "def mediapipe_detection(image, model):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
        "    image.flags.writeable = False                  # Image is no longer writeable\n",
        "    results = model.process(image)                 # Make prediction\n",
        "    image.flags.writeable = True                   # Image is now writeable\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
        "    return image, results\n",
        "\n",
        "def draw_landmarks(image, results):\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONTOURS) # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
        "\n",
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "\n",
        "def draw_landmarks(landmarks,image,show_pose=True,show_face_contour=True,show_face_tesselation=True,show_left_hand=True,show_right_hand=True):\n",
        "    annotated_image = image.copy()\n",
        "    results = landmarks\n",
        "    if show_face_tesselation:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.face_landmarks,\n",
        "            mp_holistic.FACEMESH_TESSELATION,\n",
        "            landmark_drawing_spec=None,\n",
        "            connection_drawing_spec=mp_drawing_styles\n",
        "            .get_default_face_mesh_tesselation_style())\n",
        "    if show_face_contour:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.face_landmarks,\n",
        "            mp_holistic.FACEMESH_CONTOURS,\n",
        "            landmark_drawing_spec=None,\n",
        "            connection_drawing_spec=mp_drawing_styles\n",
        "            .get_default_face_mesh_contours_style())\n",
        "    if show_pose:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.pose_landmarks,\n",
        "            mp_holistic.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing_styles.\n",
        "            get_default_pose_landmarks_style())\n",
        "    if show_left_hand:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.left_hand_landmarks,\n",
        "            mp_holistic.HAND_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing_styles\n",
        "            .get_default_hand_landmarks_style())\n",
        "    if show_right_hand:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.right_hand_landmarks,\n",
        "            mp_holistic.HAND_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing_styles\n",
        "            .get_default_hand_landmarks_style())\n",
        "    return annotated_image\n",
        "\n",
        "def display_image(img):\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Turn off the axis\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwOF_JFGCu2"
      },
      "source": [
        "## Javascript code taken from a previous Module Lab to display and get frames from a webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaJd_KkxZXr1"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  rgb_image = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Convert an RGB array to a PNG byte stream with PIL\n",
        "  bbox_pil = PIL.Image.fromarray(rgb_image, 'RGB')\n",
        "  iobuf = io.BytesIO()\n",
        "  bbox_pil.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nJXNlCvGCu2"
      },
      "source": [
        "## Inference Transformer Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DYEgmslE9rr"
      },
      "outputs": [],
      "source": [
        "SEQUENCE = []\n",
        "TenDataFrame = []\n",
        "\n",
        "def transform(results, frame_number):\n",
        "  frame = []\n",
        "  type_ = []\n",
        "  index = []\n",
        "  x = []\n",
        "  y = []\n",
        "  z = []\n",
        "  #image.flags.writeable = False\n",
        "  #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  #results = holistic.process(image)\n",
        "  #face\n",
        "  if(results.face_landmarks is None):\n",
        "    for ind in range(468):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"face\")\n",
        "      index.append(ind)\n",
        "      x.append(np.nan)\n",
        "      y.append(np.nan)\n",
        "      z.append(np.nan)\n",
        "  else:\n",
        "    for ind,val in enumerate(results.face_landmarks.landmark):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"face\")\n",
        "      index.append(ind)\n",
        "      x.append(val.x)\n",
        "      y.append(val.y)\n",
        "      z.append(val.z)\n",
        "\n",
        "  #left hand\n",
        "  if(results.left_hand_landmarks is None):\n",
        "    for ind in range(21):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"left_hand\")\n",
        "      index.append(ind)\n",
        "      x.append(np.nan)\n",
        "      y.append(np.nan)\n",
        "      z.append(np.nan)\n",
        "  else:\n",
        "    for ind,val in enumerate(results.left_hand_landmarks.landmark):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"left_hand\")\n",
        "      index.append(ind)\n",
        "      x.append(val.x)\n",
        "      y.append(val.y)\n",
        "      z.append(val.z)\n",
        "\n",
        "  #pose\n",
        "  if(results.pose_landmarks is None):\n",
        "    for ind in range(33):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"pose\")\n",
        "      index.append(ind)\n",
        "      x.append(np.nan)\n",
        "      y.append(np.nan)\n",
        "      z.append(np.nan)\n",
        "  else:\n",
        "    for ind,val in enumerate(results.pose_landmarks.landmark):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"pose\")\n",
        "      index.append(ind)\n",
        "      x.append(val.x)\n",
        "      y.append(val.y)\n",
        "      z.append(val.z)\n",
        "\n",
        "  #right hand\n",
        "  if(results.right_hand_landmarks is None):\n",
        "    for ind in range(21):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"right_hand\")\n",
        "      index.append(ind)\n",
        "      x.append(np.nan)\n",
        "      y.append(np.nan)\n",
        "      z.append(np.nan)\n",
        "  else:\n",
        "    for ind,val in enumerate(results.right_hand_landmarks.landmark):\n",
        "      frame.append(frame_number)\n",
        "      type_.append(\"right_hand\")\n",
        "      index.append(ind)\n",
        "      x.append(val.x)\n",
        "      y.append(val.y)\n",
        "      z.append(val.z)\n",
        "  #data = np.array([frame, type_, index, x, y, z])\n",
        "  return pd.DataFrame({\"frame\" : frame,\"type\"  : type_,\"landmark_index\" : index,\"x\" : x,\"y\" : y,\"z\" : z})\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Tensorflow layer to process data in TFLite\n",
        "    Data needs to be processed in the model itself, so we can not use Python\n",
        "\"\"\"\n",
        "class PreprocessLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PreprocessLayer, self).__init__()\n",
        "        normalisation_correction = tf.constant([\n",
        "                    # Add 0.50 to left hand (original right hand) and substract 0.50 of right hand (original left hand)\n",
        "                    [0] * len(LIPS_IDXS) + [0.50] * len(LEFT_HAND_IDXS) + [0.50] * len(POSE_IDXS),\n",
        "                    # Y coordinates stay intact\n",
        "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
        "                    # Z coordinates stay intact\n",
        "                    [0] * len(LANDMARK_IDXS_LEFT_DOMINANT0),\n",
        "                ],\n",
        "                dtype=tf.float32,\n",
        "            )\n",
        "        self.normalisation_correction = tf.transpose(normalisation_correction, [1,0])\n",
        "\n",
        "    def pad_edge(self, t, repeats, side):\n",
        "        if side == 'LEFT':\n",
        "            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n",
        "        elif side == 'RIGHT':\n",
        "            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=(tf.TensorSpec(shape=[None,N_ROWS,N_DIMS], dtype=tf.float32),),\n",
        "    )\n",
        "    def call(self, data0):\n",
        "        # Number of Frames in Video\n",
        "        N_FRAMES0 = tf.shape(data0)[0]\n",
        "\n",
        "        # Find dominant hand by comparing summed absolute coordinates\n",
        "        left_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1))\n",
        "        right_hand_sum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1))\n",
        "        left_dominant = left_hand_sum >= right_hand_sum\n",
        "\n",
        "        # Count non NaN Hand values in each frame for the dominant hand\n",
        "        if left_dominant:\n",
        "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
        "                    tf.where(tf.math.is_nan(tf.gather(data0, LEFT_HAND_IDXS0, axis=1)), 0, 1),\n",
        "                    axis=[1, 2],\n",
        "                )\n",
        "        else:\n",
        "            frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
        "                    tf.where(tf.math.is_nan(tf.gather(data0, RIGHT_HAND_IDXS0, axis=1)), 0, 1),\n",
        "                    axis=[1, 2],\n",
        "                )\n",
        "\n",
        "        # Find frames indices with coordinates of dominant hand\n",
        "        non_empty_frames_idxs = tf.where(frames_hands_non_nan_sum > 0)\n",
        "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
        "        # Filter frames\n",
        "        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n",
        "\n",
        "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
        "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32)\n",
        "        # Normalize to start with 0\n",
        "        non_empty_frames_idxs -= tf.reduce_min(non_empty_frames_idxs)\n",
        "\n",
        "        # Number of Frames in Filtered Video\n",
        "        N_FRAMES = tf.shape(data)[0]\n",
        "\n",
        "        # Gather Relevant Landmark Columns\n",
        "        if left_dominant:\n",
        "            data = tf.gather(data, LANDMARK_IDXS_LEFT_DOMINANT0, axis=1)\n",
        "        else:\n",
        "            data = tf.gather(data, LANDMARK_IDXS_RIGHT_DOMINANT0, axis=1)\n",
        "            data = (\n",
        "                    self.normalisation_correction + (\n",
        "                        (data - self.normalisation_correction) * tf.where(self.normalisation_correction != 0, -1.0, 1.0))\n",
        "                )\n",
        "\n",
        "        # Video fits in INPUT_SIZE\n",
        "        if N_FRAMES < INPUT_SIZE:\n",
        "            # Pad With -1 to indicate padding\n",
        "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
        "            # Pad Data With Zeros\n",
        "            data = tf.pad(data, [[0, INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
        "            # Fill NaN Values With 0\n",
        "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
        "            return data, non_empty_frames_idxs\n",
        "        # Video needs to be downsampled to INPUT_SIZE\n",
        "        else:\n",
        "            # Repeat\n",
        "            if N_FRAMES < INPUT_SIZE**2:\n",
        "                repeats = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, N_FRAMES0)\n",
        "                data = tf.repeat(data, repeats=repeats, axis=0)\n",
        "                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n",
        "\n",
        "            # Pad To Multiple Of Input Size\n",
        "            pool_size = tf.math.floordiv(len(data), INPUT_SIZE)\n",
        "            if tf.math.mod(len(data), INPUT_SIZE) > 0:\n",
        "                pool_size += 1\n",
        "\n",
        "            if pool_size == 1:\n",
        "                pad_size = (pool_size * INPUT_SIZE) - len(data)\n",
        "            else:\n",
        "                pad_size = (pool_size * INPUT_SIZE) % len(data)\n",
        "\n",
        "            # Pad Start/End with Start/End value\n",
        "            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
        "            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
        "            if tf.math.mod(pad_size, 2) > 0:\n",
        "                pad_right += 1\n",
        "\n",
        "            # Pad By Concatenating Left/Right Edge Values\n",
        "            data = self.pad_edge(data, pad_left, 'LEFT')\n",
        "            data = self.pad_edge(data, pad_right, 'RIGHT')\n",
        "\n",
        "            # Pad Non Empty Frame Indices\n",
        "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n",
        "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n",
        "\n",
        "            # Reshape to Mean Pool\n",
        "            data = tf.reshape(data, [INPUT_SIZE, -1, N_COLS, N_DIMS])\n",
        "            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [INPUT_SIZE, -1])\n",
        "\n",
        "            # Mean Pool\n",
        "            data = tf.experimental.numpy.nanmean(data, axis=1)\n",
        "            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n",
        "\n",
        "            # Fill NaN Values With 0\n",
        "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
        "\n",
        "            return data, non_empty_frames_idxs\n",
        "\n",
        "preprocess_layer = PreprocessLayer()\n",
        "\n",
        "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
        "def load_and_preprocess_data(data, preprocess_layer):\n",
        "    # Load data\n",
        "    data_columns = ['x', 'y', 'z']\n",
        "    data = data[data_columns]\n",
        "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
        "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
        "    # Apply preprocessing using the PreprocessLayer\n",
        "    processed_data = preprocess_layer(data.astype(np.float32))\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot6im-JhGCu2"
      },
      "source": [
        "## Inference LSTM Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jJVMr78GCu2"
      },
      "outputs": [],
      "source": [
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([pose, face, lh, rh])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--zBO-W8Z5m7"
      },
      "outputs": [],
      "source": [
        "\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Waiting for the first 24 frames...'\n",
        "\n",
        "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=0)\n",
        "\n",
        "bbox = ''\n",
        "count = 0\n",
        "frame_time = 0\n",
        "frame_count = 0\n",
        "sequence = []\n",
        "printed = False\n",
        "vframe_number = 0\n",
        "combine_df = pd.DataFrame()\n",
        "fps = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    vframe_number += 1\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Transformer pre processing\n",
        "    image, results = mediapipe_detection(frame, holistic)\n",
        "    testdf =  transform(results, vframe_number)\n",
        "    combine_df = pd.concat([combine_df, testdf])\n",
        "\n",
        "    # LSTM pre processing\n",
        "    keypoints = extract_keypoints(results)\n",
        "    sequence.append(keypoints)\n",
        "    sequence = sequence[-10:]\n",
        "\n",
        "    if vframe_number == 24:\n",
        "\n",
        "        if not printed:\n",
        "          print('predicting...')\n",
        "          printed = True\n",
        "\n",
        "        # LSTM Prediction\n",
        "        predictions = model_lstm.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
        "        max_confidence = np.max(predictions)\n",
        "        lstm_predicted_action = actions[np.argmax(predictions)]\n",
        "\n",
        "        # Transformer Inference\n",
        "        processed_data, non_empty_frame_idxs = load_and_preprocess_data(combine_df, preprocess_layer)\n",
        "        X = np.zeros([1, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n",
        "        NON_EMPTY_FRAME_IDXS = np.full([1, INPUT_SIZE], -1, dtype=np.float32)\n",
        "        X[0] = processed_data\n",
        "        NON_EMPTY_FRAME_IDXS[0] = non_empty_frame_idxs\n",
        "        predicted = model_transformer.predict({ 'frames': X, 'non_empty_frame_idxs': NON_EMPTY_FRAME_IDXS }, verbose=0).argmax(axis=1)\n",
        "        transformer_predicted_action = class_names[predicted[0]]\n",
        "        combine_df = pd.DataFrame()\n",
        "        label_html = f\"Predicted Action: Transformer: {transformer_predicted_action}, LSTM: {lstm_predicted_action}\"\n",
        "        vframe_number = 0\n",
        "\n",
        "    draw_styled_landmarks(image, results)\n",
        "    bbox_bytes = bbox_to_bytes(image)\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
